{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ArthurNazarenko/nebius_academy_practice/blob/main/topic2/2.1_structured_inputs_and_outputs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LLM Engineering Essentials by Nebius Academy\n",
        "\n",
        "Course github: [link](https://github.com/Nebius-Academy/LLM-Engineering-Essentials/tree/main)\n",
        "\n",
        "Author: Alex Umnov\n",
        "\n",
        "Links:\n",
        "- [LinkedIn](www.linkedin.com/in/alex-umnov)\n",
        "- Discord Profile: *alexumnov* , best to tag at #nebius-academy\n",
        "\n",
        "The course is in development now, with more materials coming soon. [Subscribe to stay updated](https://academy.nebius.com/llm-engineering-essentials/update/)\n",
        "# 2.1. Structured Inputs and Outputs\n",
        "\n",
        "In Topic 1, we learnt how to prompt an LLM in such a way that it understands what you want from it and gives a relevant answer. In this notebook we'll continue this discussion by understanding\n",
        "\n",
        "* How to make prompts reusable by using **prompt templates**\n",
        "* How to ensure that an LLM creates its outputs in a convenient, easily parsable format\n",
        "\n",
        "Let's start by running some code which will help us in the whole notebook:"
      ],
      "metadata": {
        "id": "XXuQF3YXe0C_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai -qU"
      ],
      "metadata": {
        "id": "vmxtcQ2de0DB"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "nebius_api_key = userdata.get('nebius_api_key')"
      ],
      "metadata": {
        "id": "gzwEGlR3-aHh"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "from openai import OpenAI\n",
        "import os\n",
        "\n",
        "os.environ['NEBIUS_API_KEY'] = userdata.get(\"nebius_api_key\")\n",
        "\n",
        "nebius_client = OpenAI(\n",
        "    base_url=\"https://api.studio.nebius.ai/v1/\",\n",
        "    api_key=os.environ.get(\"NEBIUS_API_KEY\"),\n",
        ")\n",
        "\n",
        "llama_model = \"meta-llama/Llama-3.3-70B-Instruct\"\n",
        "\n",
        "def prettify_string(text, max_line_length=80):\n",
        "    \"\"\"Prints a string with line breaks at spaces to prevent horizontal scrolling.\n",
        "\n",
        "    Args:\n",
        "        text: The string to print.\n",
        "        max_line_length: The maximum length of each line.\n",
        "    \"\"\"\n",
        "\n",
        "    output_lines = []\n",
        "    lines = text.split(\"\\n\")\n",
        "    for line in lines:\n",
        "        current_line = \"\"\n",
        "        words = line.split()\n",
        "        for word in words:\n",
        "            if len(current_line) + len(word) + 1 <= max_line_length:\n",
        "                current_line += word + \" \"\n",
        "            else:\n",
        "                output_lines.append(current_line.strip())\n",
        "                current_line = word + \" \"\n",
        "        output_lines.append(current_line.strip())  # Append the last line\n",
        "    return \"\\n\".join(output_lines)\n",
        "\n",
        "def answer_with_llm(prompt: str,\n",
        "                    system_prompt=\"You are a helpful assistant\",\n",
        "                    max_tokens=512,\n",
        "                    client=nebius_client,\n",
        "                    model=llama_model,\n",
        "                    prettify=True,\n",
        "                    temperature=None) -> str:\n",
        "\n",
        "    messages = []\n",
        "\n",
        "    if system_prompt:\n",
        "        messages.append(\n",
        "            {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": system_prompt\n",
        "            }\n",
        "        )\n",
        "\n",
        "    messages.append(\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": prompt\n",
        "        }\n",
        "    )\n",
        "\n",
        "    completion = client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=messages,\n",
        "        max_tokens=max_tokens,\n",
        "        temperature=temperature\n",
        "    )\n",
        "\n",
        "    if prettify:\n",
        "        return prettify_string(completion.choices[0].message.content)\n",
        "    else:\n",
        "        return completion.choices[0].message.content\n"
      ],
      "metadata": {
        "id": "a1AxEa78e0DB"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prompt templates"
      ],
      "metadata": {
        "id": "L3wUdp21e0DC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In an LLM-powered system, there's always a layer of prompting logic hidden from the user. For example, ChatGPT, Claude, Gemini and others have quite elaborate **system prompts** that set up rules and guardrails of LLM's communication with the user.\n",
        "\n",
        "However, in some cases a system prompting isn't a flexible enough mechanism. Imagine, for example,\n",
        "\n",
        "* a customer support bot that needs to be aware of the user's geography to give relevant answers about locally available products\n",
        "* a railway service support bot that needs to be aware of today's railway strikes and other calamities\n",
        "\n",
        "You'll likely need to insert this information in the middle of the prompt; and for such things, **prompt templates** are a great tool."
      ],
      "metadata": {
        "id": "xeHMXwGEe0DC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Basically, a **prompt template** is a template string like\n",
        "\n",
        "```python\n",
        "\"some fixed information {template placeholder 1}\n",
        "some more fixed information {template placeholder 2}\"\n",
        "```\n",
        "\n",
        "where the template placeholders are to be filled in just before an actual LLM call.\n",
        "\n",
        "Let's check several neat ways of wrapping this logic.\n",
        "\n",
        "First of all, you can write your own wrapper. In the example below, `m['content'].format(**kwargs)` allows to put as much formatting as you wish into the user's message."
      ],
      "metadata": {
        "id": "6i-MRNQMZvWx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List, Dict\n",
        "\n",
        "class MessagesPromptTemplate():\n",
        "    messages: List[Dict]\n",
        "\n",
        "    def __init__(self, messages: List[Dict]):\n",
        "        self.messages = messages\n",
        "\n",
        "    def format(self, **kwargs):\n",
        "        return [\n",
        "            {\n",
        "                \"role\":  m['role'],\n",
        "                \"content\": m['content'].format(**kwargs)\n",
        "            }\n",
        "            for m in self.messages\n",
        "        ]"
      ],
      "metadata": {
        "id": "ZCv8sZ5Be0DC"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_template = MessagesPromptTemplate(\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"You only answer in rhymes\"},\n",
        "        {\"role\": \"user\", \"content\": \"Tell me about {city}\"}\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "wrNIeKu3e0DD"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_template.format(city=\"Paris\")"
      ],
      "metadata": {
        "id": "kjL-E-bGe0DD",
        "outputId": "4cf6d9e0-7ffb-4675-a7c5-2b7c4a6d75f1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'role': 'system', 'content': 'You only answer in rhymes'},\n",
              " {'role': 'user', 'content': 'Tell me about Paris'}]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's try calling an llm with different variables"
      ],
      "metadata": {
        "id": "jspgVhL0e0DD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = nebius_client.chat.completions.create(\n",
        "    messages=prompt_template.format(city=\"Paris\"),\n",
        "    model=llama_model\n",
        ").choices[0].message.content\n",
        "print(outputs)"
      ],
      "metadata": {
        "id": "DuR5IqTGe0DD",
        "outputId": "5077e4c8-286b-41c3-faf1-2bf931ed5353",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In Paris, the city of love and light,\n",
            "The Eiffel Tower shines with pure delight.\n",
            "The Seine River flows, a gentle stream,\n",
            "Through streets of charm, and a romantic dream.\n",
            "\n",
            "The Louvre Museum, a treasure to see,\n",
            "Holds artwork and history, for you and me.\n",
            "The Notre Dame Cathedral, a sight to behold,\n",
            "A masterpiece of Gothic, with stories untold.\n",
            "\n",
            "Montmartre's hills, with artists so fine,\n",
            "Create masterpieces, with colors divine.\n",
            "The Champs-Élysées, a boulevard so grand,\n",
            "A shopping and dining, in this lovely land.\n",
            "\n",
            "In Paris, the city of love and desire,\n",
            "You'll find romance, and a heart on fire.\n",
            "So come and visit, this city so fair,\n",
            "And let the magic, of Paris, be beyond compare.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = nebius_client.chat.completions.create(\n",
        "    messages=prompt_template.format(city=\"Amsterdam\"),\n",
        "    model=llama_model,\n",
        ").choices[0].message.content\n",
        "print(outputs)"
      ],
      "metadata": {
        "id": "_qP1h3ahe0DE",
        "outputId": "67bebbb6-0a81-4ca5-976a-f1e240e257d8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Amsterdam's a city so fine,\n",
            "With canals and bridges that truly shine.\n",
            "The Rijksmuseum's a sight to behold,\n",
            "With art and history that never grows old.\n",
            "\n",
            "The Anne Frank House is a must-see place,\n",
            "A somber reminder of a troubled pace.\n",
            "The city's vibrant, with life and with zest,\n",
            "From coffee shops to flower markets, it's truly the best.\n",
            "\n",
            "The Jordaan neighborhood's a charming delight,\n",
            "With narrow streets and quaint shops in sight.\n",
            "The city's famous for its red light district too,\n",
            "But there's more to Amsterdam than just that to do.\n",
            "\n",
            "From bicycling through the city's streets so wide,\n",
            "To taking a boat tour, with the wind as your guide.\n",
            "Amsterdam's a city that's full of surprise,\n",
            "A destination that's sure to open your eyes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The prompt template class we've written is very primitive and would fail if, for example, some keys aren't inputted.\n",
        "\n",
        "One of the good implementations of prompt templates can be found in LangChain [PromptTemplates](https://python.langchain.com/docs/concepts/prompt_templates/)"
      ],
      "metadata": {
        "id": "3nSqLBC2e0DE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain -qU"
      ],
      "metadata": {
        "id": "m6nkay_qe0DE"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "prompt_template = ChatPromptTemplate([\n",
        "    (\"system\", \"You only answer in rhymes\"),\n",
        "    (\"user\", \"Tell me about {city}\")\n",
        "])\n",
        "\n",
        "prompt_template.invoke({\"city\": \"Madrid\"})"
      ],
      "metadata": {
        "id": "GeZEj5pCe0DE",
        "outputId": "7b3065e4-5dbb-4482-e3aa-243ad2a1eabc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatPromptValue(messages=[SystemMessage(content='You only answer in rhymes', additional_kwargs={}, response_metadata={}), HumanMessage(content='Tell me about Madrid', additional_kwargs={}, response_metadata={})])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note:** You don't have to use LangChain llm calls or anything else, you can only take their PromptTemplate implementation.\n",
        "\n",
        "However, there's quiet a bit of useful code in that library."
      ],
      "metadata": {
        "id": "5zweywyae0DE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import convert_to_openai_messages"
      ],
      "metadata": {
        "id": "8SyzAiTge0DF"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "templated_messages = convert_to_openai_messages(prompt_template.invoke({\"city\": \"Madrid\"}).to_messages())\n",
        "templated_messages"
      ],
      "metadata": {
        "id": "xvE2qTQce0DF",
        "outputId": "1ed5dd12-a8a3-44d1-a412-fab92ba1e127",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'role': 'system', 'content': 'You only answer in rhymes'},\n",
              " {'role': 'user', 'content': 'Tell me about Madrid'}]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = nebius_client.chat.completions.create(\n",
        "    messages=templated_messages,\n",
        "    model=llama_model,\n",
        ").choices[0].message.content\n",
        "print(outputs)"
      ],
      "metadata": {
        "id": "awT49JQve0DF",
        "outputId": "437615d9-c893-4d06-c4c0-1c34ed70ded8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In Madrid, the city's so fair,\n",
            "A destination that's beyond compare.\n",
            "From tapas to siestas, it's a delight,\n",
            "A place to visit, morning, noon, and night.\n",
            "\n",
            "The Prado Museum's a must-see, don't you know,\n",
            "With art and history, it's a treasure to show.\n",
            "The Royal Palace's grand, a sight to behold,\n",
            "And the Retiro Park's where locals go to unfold.\n",
            "\n",
            "The nightlife's vibrant, with music and cheer,\n",
            "And the food's delicious, with flavors so clear.\n",
            "From paella to gazpacho, it's a culinary dream,\n",
            "In Madrid, your taste buds will start to beam.\n",
            "\n",
            "So pack your bags, and come to this place,\n",
            "Madrid's waiting for you, with a warm and friendly face.\n",
            "You'll dance, you'll sing, you'll have so much fun,\n",
            "In Madrid, the city that's number one!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Structuring LLM outputs\n",
        "\n",
        "In many cases you require not just a free text answer, but something particular you can use later in your system. For example, if you want your LLM to classify a customet's intent to later pass the conversation to a relevant department, you need to extract the particular intent class from the LLM's answer.\n",
        "\n",
        "To parse your LLM outputs conveniently, it's wise to structure them in a specific way. We've already discussed some prompting tricks in Topic 1; this time, we'll learn several more reliable ways of making the LLM abide a deisgnated output format."
      ],
      "metadata": {
        "id": "PLtqUw9ce0DF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Basic output structuring\n",
        "\n",
        "As a basic way to structure your output, you can \"ask\" an LLM to present the output in a specific format. For example:"
      ],
      "metadata": {
        "id": "Wb8TpmGZe0DF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = nebius_client.chat.completions.create(\n",
        "    messages=[{\n",
        "        'role': 'user',\n",
        "        'content': \"\"\"Design one role play character\\'s name, class and a short description.\n",
        "Present it as a markdown list\"\"\"}],\n",
        "    model=llama_model,\n",
        ").choices[0].message.content\n",
        "print(outputs)"
      ],
      "metadata": {
        "id": "liGqj3PKe0DF",
        "outputId": "cc432070-4841-424f-ff00-81312d7fba09",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "* **Character Name:** Eira Shadowglow\n",
            "* **Class:** Moonlit Ranger\n",
            "* **Description:** A skilled and deadly ranger with unparalleled accuracy in the night, Eira uses her knowledge of the wilderness and her mystical connection to the moon to track and hunt her prey, bringing justice to those who dwell in the shadows.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "While this is quite good, it's not very reliable. A better way would be to show some examples to LLM so that it knows what we expect.\n",
        "\n",
        "These examples are known as **few-shot examples** and the prompting technique itself - as **few-shot prompting**."
      ],
      "metadata": {
        "id": "y7U9nI3Ae0DG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = nebius_client.chat.completions.create(\n",
        "    messages=[\n",
        "        {\n",
        "            'role': 'user',\n",
        "            'content': 'Design one role play character\\'s name, class and a short description. Present it as a markdown list.\\n'\\\n",
        "            \"Examples:\\n\"\\\n",
        "            \"\\n\"\\\n",
        "            \"- **Name:** Randalf the Yellow;\\n\"\\\n",
        "            \"- **Class:** Fire mage;\\n\"\\\n",
        "            \"- **Proficiency:** Pyro magic;\\n\"\\\n",
        "            \"- **Resistance:** Fire;\\n\"\\\n",
        "            \"\\n\"\\\n",
        "            \"- **Name:** Bonan;\\n\"\\\n",
        "            \"- **Class:** Barbarian;\\n\"\\\n",
        "            \"- **Proficiency:** Axe;\\n\"\\\n",
        "            \"- **Resistance:** Mental magic;\\n\"\\\n",
        "        }\n",
        "    ],\n",
        "    model=llama_model,\n",
        ").choices[0].message.content\n",
        "print(outputs)"
      ],
      "metadata": {
        "id": "I5lonLame0DG",
        "outputId": "c2310d94-64f8-449c-eb75-e388c0faebf6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- **Name:** Eirakai Moonwhisper;\n",
            "- **Class:** Shadow Assassin;\n",
            "- **Proficiency:** Stealth and Dual Daggers;\n",
            "- **Resistance:** Poison;\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see, LLM captured the format pretty well.\n"
      ],
      "metadata": {
        "id": "PzygVBe0e0DG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = nebius_client.chat.completions.create(\n",
        "    messages=[\n",
        "        {\n",
        "            'role': 'user',\n",
        "            'content': 'Solve the following equation and output only the answer number without reasoning after \"Answer:\"\\n' \\\n",
        "            '123 * 321 = ?\\n' \\\n",
        "            'Answer:'\n",
        "        }\n",
        "    ],\n",
        "    model=llama_model,\n",
        ").choices[0].message.content\n",
        "print(outputs)"
      ],
      "metadata": {
        "id": "S4KXsxtZe0DG",
        "outputId": "902f205a-924f-40ce-d97f-4ae755199dbb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "39543\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ever though the answer isn't correct (LLMs are notoriously bad at arithmetics), the output structure is correct and easy to parse out."
      ],
      "metadata": {
        "id": "BtVmakLFe0DG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "However, we can do even better."
      ],
      "metadata": {
        "id": "JNQ0ns6Oe0DG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Structured outputs\n",
        "\n",
        "Modern LLMs support outputing in a specific format, for example we can use \"JSON mode\" to force outputs to be in JSON fromat."
      ],
      "metadata": {
        "id": "_2G1O-w6e0DG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "json_output = nebius_client.chat.completions.create(\n",
        "    messages=[{'role': 'user', 'content': 'Design a role play character\\'s name, class and a short description in json format'}],\n",
        "    model=llama_model,\n",
        "    response_format={\"type\": \"json_object\"}\n",
        ").choices[0].message.content\n",
        "json_output"
      ],
      "metadata": {
        "id": "Q88QagE-e0DG",
        "outputId": "a5628451-bafa-44c6-816f-5d2d065fd8d2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'{\"name\": \"Eryndor Thorne\", \"class\": \"Shadow Weaver\", \"description\": \"A mysterious rogue with unparalleled stealth and agility, Eryndor navigates the shadows to manipulate the fabric of reality and outmaneuver his foes.\"}'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is useful, because that'll make it much easier for you later to parse the outputs:"
      ],
      "metadata": {
        "id": "m4KVrmlFe0DH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "json.loads(json_output)"
      ],
      "metadata": {
        "id": "b9-7ouAie0DH",
        "outputId": "1e8e6248-9775-43ae-93e2-c0b48a21d18e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'name': 'Eryndor Thorne',\n",
              " 'class': 'Shadow Weaver',\n",
              " 'description': 'A mysterious rogue with unparalleled stealth and agility, Eryndor navigates the shadows to manipulate the fabric of reality and outmaneuver his foes.'}"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can go another step further and actually define a `pydantic` model to create a schema for our outputs:"
      ],
      "metadata": {
        "id": "tTrOKdoee0DH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List\n",
        "from pydantic import BaseModel\n",
        "\n",
        "class CharacterProfile(BaseModel):\n",
        "    name: str\n",
        "    age: int\n",
        "    special_skills: List[str]\n",
        "    traits: List[str]\n",
        "    character_class: str\n",
        "    origin: str\n",
        "\n",
        "completion = nebius_client.chat.completions.create(\n",
        "    model=llama_model,\n",
        "    messages=[\n",
        "        {\"role\": \"user\", \"content\": \"Design a role play character\"}\n",
        "    ],\n",
        "    extra_body={\n",
        "        \"guided_json\": CharacterProfile.model_json_schema()\n",
        "    }\n",
        ")\n",
        "\n",
        "CharacterProfile.model_validate_json(completion.choices[0].message.content)"
      ],
      "metadata": {
        "id": "dki1ECRee0DH",
        "outputId": "4e700d7c-7c2b-411e-90a0-ef9e99fcddd1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CharacterProfile(name='Eira Shadowglow', age=25, special_skills=['Stealth', 'Archery', 'Poison-making'], traits=['Agile', 'Intelligent', 'Independent'], character_class='Rogue', origin='Moonlit Forest')"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "So no we have predefined format of outputs, which is easy to work with."
      ],
      "metadata": {
        "id": "2viyoMVOe0DH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Another way to structure outputs is using examples\n",
        "\n",
        "Let's consider an example from a famous [MMLU dataset](https://huggingface.co/datasets/cais/mmlu):"
      ],
      "metadata": {
        "id": "TpeDDvNse0DH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"Which of the following statements about Ethernets is typically FALSE?\"\n",
        "\n",
        "A = \"Ethernets use circuit switching to send messages.\"\n",
        "B = \"Ethernets use buses with multiple masters.\"\n",
        "C = \"Ethernet protocols use a collision-detection method to ensure that messages are transmitted properly.\"\n",
        "D = \"Networks connected by Ethernets are limited in length to a few hundred meters.\"\n",
        "\n",
        "correct_answer = \"A\""
      ],
      "metadata": {
        "id": "tygKuanMe0DI"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ideally we want our LLM to solve this \"test\" by answering to us with a letter corresponding to the right answer. This will also make calculating metrics much easier. Let's see what would happen."
      ],
      "metadata": {
        "id": "s5vvF33Ve0DI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output = nebius_client.chat.completions.create(\n",
        "    messages=[{\n",
        "        \"role\": \"user\",\n",
        "        \"content\": f\"\"\"\n",
        "Answer the following question with one of the options listed below\n",
        "Question: {question}\n",
        "A: {A}\n",
        "B: {B}\n",
        "C: {C}\n",
        "D: {D}\n",
        "Answer:\n",
        "\"\"\"}],\n",
        "    model=llama_model,\n",
        ").choices[0].message.content\n",
        "print(output)"
      ],
      "metadata": {
        "id": "cG2GCODVe0DI",
        "outputId": "549f15c9-7d35-46ce-f052-097e465dfa41",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A: Ethernets use circuit switching to send messages.\n",
            "\n",
            "Explanation: Ethernets use packet switching, not circuit switching. Packet switching allows multiple devices to share the same communication channel, whereas circuit switching dedicates a channel to a single communication session. \n",
            "\n",
            "The other options are true:\n",
            "- B: Ethernets do use buses with multiple masters, allowing multiple devices to transmit data.\n",
            "- C: Ethernet protocols, such as CSMA/CD (Carrier Sense Multiple Access with Collision Detection), use collision detection to manage packet transmission and avoid data loss.\n",
            "- D: Networks connected by Ethernets are limited in length, typically to a few hundred meters, due to signal attenuation and other physical limitations.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see, it did output the right answer, but if we do a simple comparison, we'll get into trouble:"
      ],
      "metadata": {
        "id": "TaJNBEK7e0DI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output == correct_answer"
      ],
      "metadata": {
        "id": "hIadZJfVe0DI",
        "outputId": "db85c35a-57f6-4f33-fdbf-98240b500551",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output"
      ],
      "metadata": {
        "id": "5Bz30z07AUOQ",
        "outputId": "cf31f18e-b86f-44ba-8011-039ed8a7f333",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174
        }
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'A: Ethernets use circuit switching to send messages.\\n\\nExplanation: Ethernets use packet switching, not circuit switching. Packet switching allows multiple devices to share the same communication channel, whereas circuit switching dedicates a channel to a single communication session. \\n\\nThe other options are true:\\n- B: Ethernets do use buses with multiple masters, allowing multiple devices to transmit data.\\n- C: Ethernet protocols, such as CSMA/CD (Carrier Sense Multiple Access with Collision Detection), use collision detection to manage packet transmission and avoid data loss.\\n- D: Networks connected by Ethernets are limited in length, typically to a few hundred meters, due to signal attenuation and other physical limitations.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "correct_answer"
      ],
      "metadata": {
        "id": "GP1be1ZyAXAB",
        "outputId": "e64e89b0-59bd-4572-98e4-142127d7229b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'A'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "So let's teach our model to answer in the right way using so-called Few Shot Prompting also known as In-Context Learning. We essentially show the model some examples in the prompt to teach it in which format we want the answer to be"
      ],
      "metadata": {
        "id": "v6igGq25e0DI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output = nebius_client.chat.completions.create(\n",
        "    messages=[{\n",
        "        \"role\": \"user\",\n",
        "        \"content\": f\"\"\"\n",
        "Examples:\n",
        "Question: The IP protocol is primarily concerned with\n",
        "A: Routing packets through the network\n",
        "B: Reliable delivery of packets between directly connected machines\n",
        "C: Reliable delivery of large (multi-packet) messages between machines that are not necessarily directly connected\n",
        "D: Dealing with differences among operating system architectures\n",
        "Answer:\n",
        "A\n",
        "\n",
        "Question: Which of the following is NOT a property of bitmap graphics?\n",
        "A: Fast hardware exists to move blocks of pixels efficiently\n",
        "B: Realistic lighting and shading can be done.\n",
        "C: All line segments can be displayed as straight.\n",
        "D: Polygons can be filled with solid colors and textures.\n",
        "Answer:\n",
        "A\n",
        "\n",
        "Task:\n",
        "Answer the following question with one of the options listed below. Only ouput the answer in the same format as the examples.\n",
        "Question: {question}\n",
        "A: {A}\n",
        "B: {B}\n",
        "C: {C}\n",
        "D: {D}\n",
        "Answer:\n",
        "\"\"\"}],\n",
        "    model=llama_model,\n",
        ").choices[0].message.content\n",
        "print(output)"
      ],
      "metadata": {
        "id": "ahOoSsxhe0DI",
        "outputId": "0caa6b4a-2ad2-487f-a600-0737edbd3936",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output == correct_answer"
      ],
      "metadata": {
        "id": "_wlgxkEue0DI",
        "outputId": "49a969f5-20ac-41db-ab81-f8f185a1ce47",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We also have observed that for some models the dialog format is actually a better way to structure the Few-Shot examples"
      ],
      "metadata": {
        "id": "uwEVq78Ve0DI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output = nebius_client.chat.completions.create(\n",
        "    messages=[{\n",
        "        \"role\": \"user\",\n",
        "        \"content\": f\"\"\"\n",
        "User: Answer the following question with one of the options listed below.\n",
        "Question: The IP protocol is primarily concerned with\n",
        "A: Routing packets through the network\n",
        "B: Reliable delivery of packets between directly connected machines\n",
        "C: Reliable delivery of large (multi-packet) messages between machines that are not necessarily directly connected\n",
        "D: Dealing with differences among operating system architectures\n",
        "Answer:\n",
        "Assistant: A\n",
        "\n",
        "User: Answer the following question with one of the options listed below.\n",
        "Question: Which of the following is NOT a property of bitmap graphics?\n",
        "A: Fast hardware exists to move blocks of pixels efficiently\n",
        "B: Realistic lighting and shading can be done.\n",
        "C: All line segments can be displayed as straight.\n",
        "D: Polygons can be filled with solid colors and textures.\n",
        "Answer:\n",
        "Assistant: A\n",
        "\n",
        "User: Answer the following question with one of the options listed below.\n",
        "Question: {question}\n",
        "A: {A}\n",
        "B: {B}\n",
        "C: {C}\n",
        "D: {D}\n",
        "Answer:\n",
        "Assitant:\n",
        "\"\"\"}],\n",
        "    model=llama_model,\n",
        ").choices[0].message.content\n",
        "print(output)"
      ],
      "metadata": {
        "id": "WsQW4lNqe0DJ",
        "outputId": "b86e7b8d-ecaf-4a4b-c2f5-d928efbc9e2b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A \n",
            "\n",
            "The statement \"Ethernets use circuit switching to send messages\" is typically false. Ethernets actually use packet switching to send messages. Circuit switching is a method used in traditional telephone networks, where a dedicated circuit is established between two endpoints for the duration of the call. In contrast, Ethernets use packet switching, where data is broken into small packets and transmitted independently, allowing for more efficient use of bandwidth.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Theoretically we don't even need to show the model relevant examples if we want it to learn the output formatting"
      ],
      "metadata": {
        "id": "rClIT520e0DJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output = nebius_client.chat.completions.create(\n",
        "    messages=[{\n",
        "        \"role\": \"user\",\n",
        "        \"content\": f\"\"\"\n",
        "Question: Choose the letter A\n",
        "A: A\n",
        "B: B\n",
        "C: C\n",
        "D: D\n",
        "Answer:\n",
        "A\n",
        "\n",
        "Question: Which is the biggest number?\n",
        "A: 1\n",
        "B: 2\n",
        "C: 3\n",
        "D: 4\n",
        "Answer:\n",
        "D\n",
        "\n",
        "Answer the following question with one of the options listed below\n",
        "Question: {question}\n",
        "A: {A}\n",
        "B: {B}\n",
        "C: {C}\n",
        "D: {D}\n",
        "Answer:\n",
        "\"\"\"}],\n",
        "    model=llama_model,\n",
        ").choices[0].message.content\n",
        "print(output)"
      ],
      "metadata": {
        "id": "bUyAniI1e0DJ",
        "outputId": "813cae83-693d-45ce-ea4b-25be45217e08",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A\n",
            "\n",
            "Explanation: Ethernets use packet switching to send messages, not circuit switching. Packet switching allows multiple devices to share the same communication channel, whereas circuit switching dedicates a channel to a single connection for the duration of the transmission. \n",
            "\n",
            "The other options are true: \n",
            "- B: Ethernets do use buses with multiple masters, meaning multiple devices can initiate communications.\n",
            "- C: Ethernet protocols, such as CSMA/CD (Carrier Sense Multiple Access with Collision Detection), use collision-detection methods to manage how devices share the network and resolve conflicts when two or more devices try to transmit at the same time.\n",
            "- D: Networks connected by Ethernets are indeed limited in length, typically to a few hundred meters, due to signal degradation over distance. Repeaters or switches can be used to extend the network length.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note:** Sometimes you can confuse the model if you have examples from the distribution, which is different than your data's one. So for the best results try to match the distribution."
      ],
      "metadata": {
        "id": "dWhLwogce0DJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Function Calling\n",
        "\n",
        "We can use tools in OpenAI api as well. Let's see how we can use web search with just the api:"
      ],
      "metadata": {
        "id": "sH-D2q9te0DJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tavily-python -qU"
      ],
      "metadata": {
        "id": "ML_0cLRre0DJ"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll need a Tavily API key which you can get from [here](https://app.tavily.com/sign-in).\n",
        "\n",
        "Then either use google's secret storage or put it into a file and upload it."
      ],
      "metadata": {
        "id": "g9r9qF5Qe0DJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#os.environ['TAVILITY_API_KEY\"] = open(\".tavily_api_key\").read()\n",
        "os.environ[\"TAVILY_API_KEY\"] = userdata.get(\"tavily_api_key\")\n",
        "\n",
        "from tavily import TavilyClient\n",
        "\n",
        "tavily_client = TavilyClient()\n",
        "\n",
        "response = tavily_client.search(\"Who is Leo Messi?\", topic=\"general\")\n",
        "\n",
        "print(response['results'])"
      ],
      "metadata": {
        "id": "RjdV-AHye0DJ",
        "outputId": "2e5f176f-2c61-4d7e-fd64-fb317393ba7c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'title': 'Lionel Messi - Wikipedia', 'url': 'https://en.wikipedia.org/wiki/Lionel_Messi', 'content': 'He is the most decorated player in the history of professional football having won 45 team trophies, including twelve Big Five \"Big Five (association football)\") league titles, four UEFA Champions Leagues, two Copa Américas, and one FIFA World Cup. Messi holds the records for most European Golden Shoes (6), most goals in a calendar year (91), most goals for a single club (672, with Barcelona), most goals (474), hat-tricks (36) and assists \"Assist (association football)\") (192) in La Liga, most assists (18) and goal contributions (32) in the Copa América, most goal contributions (21) in the World Cup, most international appearances (191) and international goals (112) by a South American male, and the second-most in the latter category outright.', 'score': 0.7876867, 'raw_content': None}, {'title': 'Lionel Messi: Biography, Soccer Player, Inter Miami CF, Athlete', 'url': 'https://www.biography.com/athletes/lionel-messi', 'content': 'Lionel Messi, a forward for Inter Miami CF, is one of the world’s greatest soccer players and helped the Argentina national team win its third FIFA World Cup in 2022. Messi, now playing for Inter Miami CF of the MLS, helped his home country win soccer’s biggest event for the first time since 1986, scoring two goals in the final and leading Argentina to a 4-2 win over Kylian Mbappé and France on penalties. Lionel Messi is an Argentinian soccer player who has played for FC Barcelona, Paris Saint-Germain, and currently, the MLS club Inter Miami CF as well as the Argentina national team. lionel messi, his wife, and three youth sons walk on a soccer field, all are wearing argentina national team jerseys and messi is holding up the fifa world cup trophy, behind them is a crowd in the stands', 'score': 0.7498395, 'raw_content': None}, {'title': 'Lionel Messi Biography - Facts, Childhood, Family Life & Achievements', 'url': 'https://www.thefamouspeople.com/profiles/lionel-messi-5242.php', 'content': \"Lionel Messi Biography - Facts, Childhood, Family Life & Achievements =============== The Famous People Lists Profession Born Today Quiz Birthdays Feedback T F P Advanced Search Lists This Day In History Profession Quiz Time Born Today Died Today Quotes Recent Lionel Messi Biography (One of the Greatest Football Players of All Time) Birthday:June 24, 1987 (Cancer) Born In: Rosario, Argentina Advanced Search Lionel Messi is an Argentine former professional footballer counted amongst the best players in the world in football history. The only player in history to win five FIFA Ballons d'Or, he was also the first player to win three European Golden Shoes. During his playing career he shattered many world records and currently holds the records for most goals scored in La Liga, a La Liga season (50), a calendar year (91), a single season (73), and a Champions League match (5), among several others. Having made his competitive debut as a 17 year old, Messi quickly established himself as an integral player for the Barcelona club.\", 'score': 0.7447496, 'raw_content': None}, {'title': 'Official site', 'url': 'https://messi.com', 'content': 'Web oficial Leo Messi jugador del Inter de Miami – messi.com – Web oficial de Lionel Messi, jugador del Futbol Profesional y campeón mundial con la selección Argentina Image 1: Web oficial Leo Messi jugador del Inter de Miami – messi.com Web oficial Leo Messi jugador del Inter de Miami - messi.com DOS GOLES Y DOS ASISTENCIAS DE LEO EN LA GOLEADA ANTE COLUMBUS Leo anotó dos goles y dio dos asistencias en el triunfo 5-1 de Inter Miami ante Columbus Crew, en el […] Leo anotó dos goles y dio una asistencia en la victoria de Inter Miami por 4-2 ante Montréal en el […] Leo Messi Management S.L.U. utiliza cookies propias y de terceros para ofrecerle contenidos adaptados a sus intereses.', 'score': 0.6852132, 'raw_content': None}, {'title': \"Lionel Messi | Biography, Trophies, Records, Ballon d'Or, Inter Miami ...\", 'url': 'https://www.britannica.com/biography/Lionel-Messi', 'content': 'Lionel Messi scored 73 goals during the 2011–12 season while playing for FC Barcelona, breaking a 39-year-old record for single-season goals in a major European football league. Messi’s play continued to rapidly improve over the years, and by 2008 he was one of the most dominant players in the world, finishing second to Manchester United’sCristiano Ronaldo in the voting for the 2008 Ballon d’Or. In early 2009 Messi capped off a spectacular 2008–09 season by helping FC Barcelona capture the club’s first “treble” (winning three major European club titles in one season): the team won the La Liga championship, the Copa del Rey (Spain’s major domestic cup), and the Champions League title.', 'score': 0.5832277, 'raw_content': None}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can define a `tool` description for client, so that the model knows how to use it.\n",
        "\n",
        "We will only expose `query` and `topic` parameters.\n",
        "\n",
        "We also need to write short descriptions to explain what the tool and the parameters are for. Note that it's not for you, but for the LLM :) So please make sure you provide a clear explanation.\n",
        "\n",
        "Tool usage is sort of an extension of \"JSON mode\" because in the end we get a dict of parameters, parsed from the JSON."
      ],
      "metadata": {
        "id": "IWspcVJxe0DJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tools = [\n",
        "    {\n",
        "        \"type\": \"function\",\n",
        "        \"function\": {\n",
        "            \"name\": \"web-search\",\n",
        "            \"description\": \"Retrieves results from web search\",\n",
        "            \"parameters\": {\n",
        "                \"type\": \"object\",\n",
        "                \"properties\": {\n",
        "                    \"query\": {\n",
        "                        \"type\": \"string\",\n",
        "                        \"description\": \"What you search for\",\n",
        "                    },\n",
        "                    \"topic\": {\n",
        "                        \"type\": \"string\",\n",
        "                        \"description\": \"Search topic either 'general' or 'news'\",\n",
        "                        \"enum\": [\"general\", \"news\"]\n",
        "                    },\n",
        "                },\n",
        "                \"required\": [\"query\"],\n",
        "            },\n",
        "        }\n",
        "    },\n",
        "]\n",
        "\n",
        "\n",
        "messages = []\n",
        "messages.append({\"role\": \"system\", \"content\": \"If you are asked about the factual information, create a function call instead. If you already searched, use the results to give an answer.\"})\n",
        "messages.append({\"role\": \"user\", \"content\": \"What is the name of the cat from Shrek?\"})\n",
        "chat_response = nebius_client.chat.completions.create(\n",
        "    messages=messages, tools=tools, model=llama_model\n",
        ")\n",
        "chat_response"
      ],
      "metadata": {
        "id": "Y00ckl3Ie0DJ",
        "outputId": "288e2265-72fb-4210-9aca-8619e8364cff",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatCompletion(id='chatcmpl-9c9092ea546b4e54949911b355ccb1d6', choices=[Choice(finish_reason='tool_calls', index=0, logprobs=None, message=ChatCompletionMessage(content=None, refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='chatcmpl-tool-ae46e24d151f4cfc964095b13bcc4d6d', function=Function(arguments='{\"query\": \"Shrek cat name\", \"topic\": \"general\"}', name='web-search'), type='function')], reasoning_content=None), stop_reason=128008)], created=1749461310, model='meta-llama/Llama-3.3-70B-Instruct', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=32, prompt_tokens=267, total_tokens=299, completion_tokens_details=None, prompt_tokens_details=None), prompt_logprobs=None)"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "And we can also try to ask for some news-worthy content to see if LLM decides on a different `topic`."
      ],
      "metadata": {
        "id": "xcU7jtcoe0DK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "messages = []\n",
        "messages.append({\"role\": \"system\", \"content\": \"If you are asked about the factual information, create a function call instead. If you already searched, use the results to give an answer.\"})\n",
        "messages.append({\"role\": \"user\", \"content\": \"What happened in London today?\"})\n",
        "chat_response = nebius_client.chat.completions.create(\n",
        "    messages=messages, tools=tools, model=llama_model\n",
        ")\n",
        "chat_response"
      ],
      "metadata": {
        "id": "gx4QpV3Le0DK",
        "outputId": "70ca2f7c-1f62-422b-eebc-bc5e6428f4e0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatCompletion(id='chatcmpl-e5ad035574964c138e99334cd05c18af', choices=[Choice(finish_reason='tool_calls', index=0, logprobs=None, message=ChatCompletionMessage(content=None, refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='chatcmpl-tool-239737bdc1ba4b43b37497d51458987e', function=Function(arguments='{\"query\": \"London news today\", \"topic\": \"news\"}', name='web-search'), type='function')], reasoning_content=None), stop_reason=128008)], created=1749461343, model='meta-llama/Llama-3.3-70B-Instruct', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=31, prompt_tokens=262, total_tokens=293, completion_tokens_details=None, prompt_tokens_details=None), prompt_logprobs=None)"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can extract the function usage output from the result"
      ],
      "metadata": {
        "id": "n94smyzfe0DK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chat_response.choices[0].message.tool_calls[0]"
      ],
      "metadata": {
        "id": "-AMA-dM9e0DK",
        "outputId": "1b3f7849-ca4a-48df-d233-bbbb16a8983f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatCompletionMessageToolCall(id='chatcmpl-tool-239737bdc1ba4b43b37497d51458987e', function=Function(arguments='{\"query\": \"London news today\", \"topic\": \"news\"}', name='web-search'), type='function')"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You might be wondering, why do we include tool usage in structured output topic.\n",
        "\n",
        "Thing is, you can also use this functionality to structure your output. You don't have to use a real function as your tool. Let's use our previous example"
      ],
      "metadata": {
        "id": "hDqCaytPe0DK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tools = [\n",
        "    {\n",
        "        \"type\": \"function\",\n",
        "        \"function\": {\n",
        "            \"name\": \"create_rpg_character\",\n",
        "            \"description\": \"Creates a character based on attributes and description\",\n",
        "            \"parameters\": {\n",
        "                \"type\": \"object\",\n",
        "                \"properties\": {\n",
        "                    \"name\": {\n",
        "                        \"type\": \"string\",\n",
        "                        \"description\": \"Name of the character\",\n",
        "                    },\n",
        "                    \"age\": {\n",
        "                        \"type\": \"integer\",\n",
        "                        \"description\": \"Age of the character\",\n",
        "                    },\n",
        "                    \"special_skills\": {\n",
        "                        \"type\": \"array\",\n",
        "                        \"description\": \"List of special skills of the character\",\n",
        "                        \"items\": {\n",
        "                            \"type\": \"string\"\n",
        "                        }\n",
        "                    },\n",
        "                    \"traits\": {\n",
        "                        \"type\": \"array\",\n",
        "                        \"description\": \"List of traits of the character\",\n",
        "                        \"items\": {\n",
        "                            \"type\": \"string\"\n",
        "                        }\n",
        "                    },\n",
        "                    \"character_class\": {\n",
        "                        \"type\": \"string\",\n",
        "                        \"description\": \"Class of the character\",\n",
        "                        \"enum\": [\"mage\", \"rogue\", \"barbarian\", \"knight\", \"paladin\"]\n",
        "                    },\n",
        "                    \"origin\": {\n",
        "                        \"type\": \"string\",\n",
        "                        \"description\": \"Origin of the character\",\n",
        "                        \"enum\": [\"human\", \"elf\", \"orc\", \"undead\"]\n",
        "                    },\n",
        "                },\n",
        "                \"required\": [\"name\", \"age\", \"special_skills\", \"traits\", \"character_class\", \"origin\"],\n",
        "            },\n",
        "        }\n",
        "    },\n",
        "]\n"
      ],
      "metadata": {
        "id": "ATabJqL4e0DK"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "messages = []\n",
        "messages.append({\"role\": \"system\", \"content\": \"If you are asked to create a character, use `create_rpg_character` tool.\"})\n",
        "messages.append({\"role\": \"user\", \"content\": \"Generate a random character for my new session\"})\n",
        "chat_response = nebius_client.chat.completions.create(\n",
        "    messages=messages, tools=tools, model=llama_model\n",
        ")\n",
        "chat_response.choices[0].message.tool_calls[0].function.arguments"
      ],
      "metadata": {
        "id": "uFf2dqmPe0DK",
        "outputId": "30792fa4-7593-40c1-f915-44a0b074053a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'{\"name\": \"Eryndor Thorne\", \"age\": \"25\", \"special_skills\": \"[\\\\\"stealth\\\\\", \\\\\"archery\\\\\"]\", \"traits\": \"[\\\\\"charismatic\\\\\", \\\\\"intelligent\\\\\"]\", \"character_class\": \"rogue\", \"origin\": \"human\"}'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Practice tasks**\n",
        "\n",
        "If you encounter any difficulties or simply want to see our solutions, feel free to check the [Solutions notebook](https://colab.research.google.com/github/Nebius-Academy/LLM-Engineering-Essentials/blob/main/topic2/2.1_structured_inputs_and_outputs_solutions.ipynb)."
      ],
      "metadata": {
        "id": "4bPMLxcse0DK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 1. LLM Information extraction\n",
        "\n",
        "The goal of this task is to create a system, which extracts data about events from free text into a predictable format."
      ],
      "metadata": {
        "id": "7ZlJaY22e0DK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's imagine that you work for a marketing agency, and you need to gather analytics about the passing events dedicated to AI and Machine Learning. For that, you need to process press releases and extract:\n",
        "- Event name,\n",
        "- Event date,\n",
        "- Number of participants,\n",
        "- Number of speakers,\n",
        "- Attendance price.\n",
        "\n",
        "Of course, you can do it manually, but it's much more fun to use Generative AI! So, your task will be to write a function that does this with only one request to OpenAI API.\n",
        "\n",
        "Below there is an example of a press release (generated by ChatGPT, of course, so that both the event and the personae are fictional). All of them are in the press_releases.zip archive in the hometask week 1 folder.\n",
        "\n",
        "<blockquote>\n",
        "<p>PRESS RELEASE\n",
        "\n",
        "InnovAI Summit 2023: A Glimpse into the Future of Artificial Intelligence</p>\n",
        "\n",
        "City of Virtue, Cyberspace - November 8, 2023 - The most anticipated event of the year, InnovAI Summit 2023, successfully concluded last weekend, on November 5, 2023. Held in the state-of-the-art VirtuTech Arena, the summit saw a massive turnout of over 3,500 participants, from brilliant AI enthusiasts and researchers to pioneers in the field.\n",
        "\n",
        "Esteemed speakers took to the stage to shed light on the latest breakthroughs, practical implementations, and ethical considerations in AI. Dr. Evelyn Quantum, renowned for her groundbreaking work on Quantum Machine Learning, emphasized the importance of this merger and how it's revolutionizing computing as we know it. Another keynote came from Prof. Leo Nexus, whose current project 'AI for Sustainability' highlights the symbiotic relationship between nature and machine, aiming to use AI in restoring our planet's ecosystems.\n",
        "\n",
        "This year's panel discussion, moderated by the talented Dr. Ada Neura, featured lively debates on the limits of AI in creative arts. Renowned digital artist, Felix Vortex, showcased how he uses generative adversarial networks to create surreal art pieces, while bestselling author, Iris Loom, explained her experiments with AI-assisted story crafting.\n",
        "\n",
        "Among other highlights were hands-on workshops, interactive Q&A sessions, and an 'AI & Ethics' debate which was particularly well-received, emphasizing the need for transparency and fairness in AI models. An exclusive 'Start-up Alley' allowed budding entrepreneurs to showcase their innovations, gaining attention from global venture capitalists and media.\n",
        "\n",
        "The event wrapped up with an announcement for InnovAI Summit 2024, set to be even grander. Participants left with a renewed enthusiasm for the vast possibilities that the AI and ML world promises.\n",
        "\n",
        "For media inquiries, please contact:\n",
        "Jane Cipher\n",
        "Director of Communications, InnovAI Summit\n",
        "Email: jane.cipher@innovai.org\n",
        "Phone: +123-4567-8910</p>\n",
        "</blockquote>\n",
        "\n",
        "More specifically, you should write a function\n",
        "\n",
        "```python\n",
        "parse_press_release(pr: str) -> dict\n",
        "```\n",
        "\n",
        "where the output should be in the format\n",
        "\n",
        "```python\n",
        "{\n",
        "  name: 'InnovAI Summit 2023',\n",
        "  date: '08.11.2023',\n",
        "  n_participants: 3500,\n",
        "  n_speakers: 4,\n",
        "  price:\n",
        "}\n",
        "```\n",
        "\n",
        "If any of the four characteristics is not mentioned in the text, put `None` in the respective field.\n",
        "\n",
        "At the end, calculate the statistics of right answers and analyse what kind of mistakes you \"model\" makes the most."
      ],
      "metadata": {
        "id": "cEvtiZy6e0DK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Hints and suggestions:**\n",
        "- It's gonna be more convenient to experiment in Nebius AI Studio's playground https://studio.nebius.com/playground.\n",
        "- You need to be very accurate with what you want from the model.\n",
        "- It will help if you specify in the prompt that the output should be in JSON format, this way you will spend less time parsing the output. But be careful. Though some models are easily prompted to output a JSON, please check the output format. It may contain excessive formatting, for example:\n",
        "<pre><code>```json\n",
        "{\"name\": \"InnovAI Summit 2023\", ...}\n",
        "```</pre></code>\n",
        "Actually, examining LLM outputs and their format is a must when working with them\n",
        "\n",
        "- Please be careful with the details. For example, Jane Cipher in the text above is not a speaker and shouldn't be counter as such (how to get rid of a contact person?). Also pay attention to the date format,\n",
        "- If the model is too wilful with the output format, don't hesitate to show some examples. Decreasing the temperature of predictions can help reduce the creativity of the answer, which is what we want for such task.\n",
        "- Debugging an LLM-powered application may become a tough business. When you think that you've polished it, an LLM can still surprise you. So, we don't expect 100% accuracy in this task, but we expect that you do your best to achieve high quality results."
      ],
      "metadata": {
        "id": "eySv4YHWe0DK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Bonus points**:\n",
        "Try writing the solution using:\n",
        "- Structured JSON Output\n",
        "- Guiding JSON Output using Structures"
      ],
      "metadata": {
        "id": "vbJYFRc6e0DK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "press_release = \"\"\"PRESS RELEASE\n",
        "\n",
        "InnovAI Summit 2023: A Glimpse into the Future of Artificial Intelligence\n",
        "\n",
        "City of Virtue, Cyberspace - November 8, 2023 - The most anticipated event of the year, InnovAI Summit 2023, successfully concluded last weekend, on November 5, 2023. Held in the state-of-the-art VirtuTech Arena, the summit saw a massive turnout of over 3,500 participants, from brilliant AI enthusiasts and researchers to pioneers in the field.\n",
        "\n",
        "Esteemed speakers took to the stage to shed light on the latest breakthroughs, practical implementations, and ethical considerations in AI. Dr. Evelyn Quantum, renowned for her groundbreaking work on Quantum Machine Learning, emphasized the importance of this merger and how it's revolutionizing computing as we know it. Another keynote came from Prof. Leo Nexus, whose current project 'AI for Sustainability' highlights the symbiotic relationship between nature and machine, aiming to use AI in restoring our planet's ecosystems.\n",
        "\n",
        "This year's panel discussion, moderated by the talented Dr. Ada Neura, featured lively debates on the limits of AI in creative arts. Renowned digital artist, Felix Vortex, showcased how he uses generative adversarial networks to create surreal art pieces, while bestselling author, Iris Loom, explained her experiments with AI-assisted story crafting.\n",
        "\n",
        "Among other highlights were hands-on workshops, interactive Q&A sessions, and an 'AI & Ethics' debate which was particularly well-received, emphasizing the need for transparency and fairness in AI models. An exclusive 'Start-up Alley' allowed budding entrepreneurs to showcase their innovations, gaining attention from global venture capitalists and media.\n",
        "\n",
        "The event wrapped up with an announcement for InnovAI Summit 2024, set to be even grander. Participants left with a renewed enthusiasm for the vast possibilities that the AI and ML world promises.\n",
        "\n",
        "For media inquiries, please contact: Jane Cipher Director of Communications, InnovAI Summit Email: jane.cipher@innovai.org Phone: +123-4567-8910\"\"\""
      ],
      "metadata": {
        "id": "zJspUelOe0DL"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import re\n",
        "\n",
        "def extract_triple_backtick_blocks(text):\n",
        "    \"\"\"\n",
        "    Extracts all text enclosed between triple backticks (```).\n",
        "    Returns a list of code/text blocks.\n",
        "    \"\"\"\n",
        "    return re.findall(r\"```(.*?)```\", text, re.DOTALL)\n",
        "\n",
        "def parse_press_release(pr: str) -> dict:\n",
        "    answer = answer_with_llm(\n",
        "            f\"Here's a press release\\n{pr}\\n\\nExtract from it the following json:\"\\\n",
        "            \"If any information needed for JSON is not available, write \\\"None\\\" instead (with quotes).\\n\"\\\n",
        "            '{\"name\": NAME_OF_EVENT, \"date\": DATE_OF_EVENT, \"n_participants\": NUM_PARTICIPANTS, \"n_speakers\": NUM_SPEAKERS, \"price\": PRICE}'\\\n",
        "            \"NAME_OF_EVENT should be the name of event advertised,\\n\"\\\n",
        "            \"DATE_OF_EVENT hould be the date of event mentioned in format DD.MM.YYYY or DD.MM.YYYY-DD.MM.YYYY if the event lasted for several days,\\n\"\\\n",
        "            \"NUM_PARTICIPANTS should be the estimated amount of participants of said event in a format like 200 or 1000 or 10000, do not write it like 2,000,\\n\"\\\n",
        "            \"NUM_SPEAKERS is a number, corresponding to amount of names of speakers and hosts mentioned\\n\"\\\n",
        "            \"PRICE should be the price of event in the format EUR 100 or USD 1000 or GBP 100 depending on currency. Do not write currency symbol, instead write an abbreviation.\\n\"\\\n",
        "            \"If any information needed for JSON is not available, write a json string \\\"None\\\" instead (with quotes).\"\n",
        "    )\n",
        "    try:\n",
        "        if \"```\" in answer:\n",
        "            answer = extract_triple_backtick_blocks(answer)[0]\n",
        "        return json.loads(answer)\n",
        "    except Exception as e:\n",
        "        print(answer)\n",
        "        raise"
      ],
      "metadata": {
        "id": "fXP5oO41e0DL"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parse_press_release(press_release)"
      ],
      "metadata": {
        "id": "fZ7Bna2JCarP",
        "outputId": "93a8d4e2-d5b5-43bc-a3db-92e37074068f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'name': 'InnovAI Summit 2023',\n",
              " 'date': '05.11.2023',\n",
              " 'n_participants': 3500,\n",
              " 'n_speakers': 5,\n",
              " 'price': 'None'}"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testing\n",
        "\n",
        "We've prepared a small dataset for you to test your prompt on. Provided you've written your function, try running the following code. At the end you also have an opportunity to look at the results in a table side-by-side in with_results.csv. Your goal is to get at least 60% of fields right.."
      ],
      "metadata": {
        "id": "rXJF_b3de0DL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade gdown\n",
        "!gdown -O press_release_extraction.csv https://docs.google.com/spreadsheets/d/15IGdc3MV8864lxrLxsug0Ij480p76T1EAwBM7WGT_OI/export?format=csv"
      ],
      "metadata": {
        "id": "dTVTe284e0DL",
        "outputId": "e6a602f0-f87c-4bce-947d-bd1bb7ab00e3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gdown in /usr/local/lib/python3.11/dist-packages (5.2.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from gdown) (4.13.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from gdown) (3.18.0)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.11/dist-packages (from gdown) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from gdown) (4.67.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->gdown) (2.7)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->gdown) (4.14.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (2025.4.26)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (1.7.1)\n",
            "/usr/local/lib/python3.11/dist-packages/gdown/parse_url.py:48: UserWarning: You specified a Google Drive link that is not the correct link to download a file. You might want to try `--fuzzy` option or the following url: https://drive.google.com/uc?id=None\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From: https://docs.google.com/spreadsheets/d/15IGdc3MV8864lxrLxsug0Ij480p76T1EAwBM7WGT_OI/export?format=csv\n",
            "To: /content/press_release_extraction.csv\n",
            "16.0kB [00:00, 23.9MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas\n",
        "pr_df = pandas.read_csv(\"press_release_extraction.csv\")\n",
        "pr_df.head()"
      ],
      "metadata": {
        "id": "OJjcT2pSe0DL",
        "outputId": "38bb8e81-b080-4597-8d50-ae2b8f0d19c9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        }
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                             pr_text  \\\n",
              "0  InnovAI Summit 2023: A Glimpse into the Future...   \n",
              "1  Press Dispatch: 'Artificial Mariners: Navigati...   \n",
              "2  FOR IMMEDIATE RELEASE\\n\\nAI Innovators Convene...   \n",
              "3  Press Release: Cutting-Edge Innovations Debute...   \n",
              "4  Press Release: Innovative Minds Gather at \"AI ...   \n",
              "\n",
              "                                           pr_parsed  \n",
              "0  {\\n  \"name\": \"InnovAI Summit 2023\",\\n  \"date\":...  \n",
              "1  {\"name\": \"Artificial Mariners: Navigatin' the ...  \n",
              "2  {\"name\": \"Annual Machine Learning Symposium 20...  \n",
              "3  {\"name\": \"AI Advancements Summit 2023\",\\n \"dat...  \n",
              "4  {\"name\": \"AI Horizon 2023\",\\n \"date\": \"15.10.2...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-4589b11a-bb5e-4838-be43-a0466efbd8b4\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>pr_text</th>\n",
              "      <th>pr_parsed</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>InnovAI Summit 2023: A Glimpse into the Future...</td>\n",
              "      <td>{\\n  \"name\": \"InnovAI Summit 2023\",\\n  \"date\":...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Press Dispatch: 'Artificial Mariners: Navigati...</td>\n",
              "      <td>{\"name\": \"Artificial Mariners: Navigatin' the ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>FOR IMMEDIATE RELEASE\\n\\nAI Innovators Convene...</td>\n",
              "      <td>{\"name\": \"Annual Machine Learning Symposium 20...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Press Release: Cutting-Edge Innovations Debute...</td>\n",
              "      <td>{\"name\": \"AI Advancements Summit 2023\",\\n \"dat...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Press Release: Innovative Minds Gather at \"AI ...</td>\n",
              "      <td>{\"name\": \"AI Horizon 2023\",\\n \"date\": \"15.10.2...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4589b11a-bb5e-4838-be43-a0466efbd8b4')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-4589b11a-bb5e-4838-be43-a0466efbd8b4 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-4589b11a-bb5e-4838-be43-a0466efbd8b4');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-0ad27642-5ea8-424c-b3c6-88bf7803eb28\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-0ad27642-5ea8-424c-b3c6-88bf7803eb28')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-0ad27642-5ea8-424c-b3c6-88bf7803eb28 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "pr_df",
              "summary": "{\n  \"name\": \"pr_df\",\n  \"rows\": 7,\n  \"fields\": [\n    {\n      \"column\": \"pr_text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 7,\n        \"samples\": [\n          \"InnovAI Summit 2023: A Glimpse into the Future of Artificial Intelligence\\n\\nCity of Virtue, Cyberspace - November 8, 2023 - The most anticipated event of the year, InnovAI Summit 2023, successfully concluded last weekend, on November 5, 2023. Held in the state-of-the-art VirtuTech Arena, the summit saw a massive turnout of over 3,500 participants, from brilliant AI enthusiasts and researchers to pioneers in the field.\\n\\nEsteemed speakers took to the stage to shed light on the latest breakthroughs, practical implementations, and ethical considerations in AI. Dr. Evelyn Quantum, renowned for her groundbreaking work on Quantum Machine Learning, emphasized the importance of this merger and how it's revolutionizing computing as we know it. Another keynote came from Prof. Leo Nexus, whose current project 'AI for Sustainability' highlights the symbiotic relationship between nature and machine, aiming to use AI in restoring our planet's ecosystems.\\n\\nThis year's panel discussion, moderated by the talented Dr. Ada Neura, featured lively debates on the limits of AI in creative arts. Renowned digital artist, Felix Vortex, showcased how he uses generative adversarial networks to create surreal art pieces, while bestselling author, Iris Loom, explained her experiments with AI-assisted story crafting.\\n\\nAmong other highlights were hands-on workshops, interactive Q&A sessions, and an 'AI & Ethics' debate which was particularly well-received, emphasizing the need for transparency and fairness in AI models. An exclusive 'Start-up Alley' allowed budding entrepreneurs to showcase their innovations, gaining attention from global venture capitalists and media.\\n\\nThe event wrapped up with an announcement for InnovAI Summit 2024, set to be even grander. Participants left with a renewed enthusiasm for the vast possibilities that the AI and ML world promises.\\n\\nFor media inquiries, please contact: Jane Cipher Director of Communications, InnovAI Summit Email: jane.cipher@innovai.org Phone: +123-4567-8910\",\n          \"Press Dispatch: 'Artificial Mariners: Navigatin' the AI Seas' - The Grand AI and Machine Learnin' Symposium of 2023\\n\\nOctober 12, 2023 - Tortuga Bay, The Spanish Main - Avast ye! Just a fortnight past, the shores of Tortuga Bay were graced with the grandest gatherin' of minds and marauders from across the seven seas. The event known far and wide as \\\"Artificial Mariners: Navigatin' the AI Seas\\\" did cast its anchor on the 8th and 9th of October, bringin' together a motley crew of over 2,000 sea dogs, scholars, and ship captains keen on decipherin' the mysteries of Artificial Intelligence and Machine Learnin'.\\n\\nWith the Jolly Roger flyin' high, the symposium boasted of tales and tools shared by the likes of the fearsome Dr. \\\"Blackwater\\\" Aria Kessler, known in the New World and Old for her dark arts in makin' machines mimic the mind. There were whispers amongst the crew about Dr. Jun \\\"Kraken\\\" Zhao, who spoke of harnessin' the monstrous power of calculations with nary a need for extra rum, savin' energy like a true sea scoundrel.\\n\\nNo gatherin' of this sort would be complete without explorin' the depths of ethical plunderin', led by the sharp-witted Dr. Sofia \\\"Siren\\\" \\u00c1lvarez. Her talk drew maps for navigatin' the fine line 'twixt progress and plunder, makin' sure the power of our newfound intelligence be used for the good of all, not just the few.\\n\\nBut shiver me timbers, it weren\\u2019t all just jabber! Tom\\u00e1s \\\"One-Eye\\\" Rivera, a cunning pirate with a penchant for codes and ciphers, put together a hands-on spectacle. This live code-crackin' session saw shipmates and buccaneers alike put their heads together, tacklin' problems that'd make even the saltiest of sea dogs sweat.\\n\\nAs the sun set on the final day, the renowned and somewhat mystical Dr. Emilia \\\"Seafarer\\\" van der Meer took to the stage, her eyes alight with visions of uncharted waters. Her words wove tales of futures where our trusty shipmates, the AI, be integral to weatherin' the storms ahead, safeguardin' not just our gold, but our lands and livelihoods.\\n\\n\\\"'Twas a rally like no other, fillin' our hearts with fire and our minds with dreams of treasures that lay beyond the horizons of man and machine,\\\" confessed an old tar as he prepared to disembark.\\n\\nAs the symposium closed its doors, the air was thick with plans and plots, the promise of alliances, and a shared quest to conquer the vast, unpredictable seas of Artificial Intelligence.\\n\\nFor those wishin' to re-live the adventure or who couldn\\u2019t sail with us this time around, visit [Symposium\\u2019s Mysterious Cove].\\n\\nContact for parley:\\nName: [Your trusty informant]\\nTitle: [Harbormaster of Information]\\nBird-mail: [Email]\\nMessage in a bottle: [Phone Number]\",\n          \"Press Release: \\\"AI for Equity Summit\\\" Champions Inclusivity in Technology\\n\\nOctober 18, 2023 \\u2014 The transformative \\\"AI for Equity Summit\\\" convened on October 15, 2023, marking an historic congregation of technological prowess dedicated to inclusivity and equitable advancements in artificial intelligence. The event welcomed over 3,000 attendees, each contributing a registration fee of $250, signifying their commitment to nurturing diversity in AI.\\n\\nSix illustrious speakers graced the summit's virtual stage, including Dr. Ayesha Khurram, renowned for her work in ethical AI, tech visionary Omar Svensson, Dr. Lola Adebayo, an advocate for minorities in STEM, Dr. Ji-hoon Park, known for his innovative algorithms against biases, Maria Navarro, a crusader for women in tech, and coding prodigy, Zane Kirschner, who is making significant strides in accessible AI for persons with disabilities.\\n\\nThe summit wasn't just about discussions; it was about making tangible strides. The highlight was the launch of the \\\"AI Equity Initiative,\\\" a fund that collected over $1.2 million from participant contributions, aimed at supporting tech education in underserved regions.\\n\\nWorkshops emphasized actionable strategies for dismantling systemic barriers in the tech industry. Dr. Adebayo's session on 'Intersectionality in AI Development' and Kirschner's workshop titled 'Coding Without Barriers' particularly stood out for their depth and engagement.\\n\\nMoreover, the \\\"AI for Equity Summit\\\" was proud to allocate 20% of all registration fees toward scholarships for students from underrepresented backgrounds to pursue AI studies.\\n\\nIn the wake of the event, a communiqu\\u00e9 was released, expressing a unified pledge among the participants and speakers to proactively include underrepresented groups in both the development of AI and the conversations around it.\\n\\nAttendees and interested parties are encouraged to stay informed through the summit's official channels for ongoing initiatives and future events.\\n\\nEnd of Release\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"pr_parsed\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 7,\n        \"samples\": [\n          \"{\\n  \\\"name\\\": \\\"InnovAI Summit 2023\\\",\\n  \\\"date\\\": \\\"05.11.2023\\\",\\n  \\\"n_participants\\\": 3500,\\n  \\\"n_speakers\\\": 4,\\n  \\\"price\\\": \\\"None\\\"\\n}\",\n          \"{\\\"name\\\": \\\"Artificial Mariners: Navigatin' the AI Seas\\\",\\n \\\"date\\\": \\\"08.10.2023-09.10.2023\\\",\\n \\\"n_participants\\\": 2000,\\n \\\"n_speakers\\\": 5,\\n \\\"price\\\":\\\"None\\\"}\",\n          \"{\\\"name\\\": \\\"AI for Equity Summit\\\",\\n \\\"date\\\": \\\"15.10.2023\\\",\\n \\\"n_participants\\\":  3000,\\n \\\"n_speakers\\\": 6,\\n \\\"price\\\": \\\"USD 250\\\"}\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pr_df.pr_parsed[0]"
      ],
      "metadata": {
        "id": "sDGJ1vKxe0DL",
        "outputId": "84c0d8a8-1083-4f13-f448-689c73e9900e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'{\\n  \"name\": \"InnovAI Summit 2023\",\\n  \"date\": \"05.11.2023\",\\n  \"n_participants\": 3500,\\n  \"n_speakers\": 4,\\n  \"price\": \"None\"\\n}'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "parsed_list = []\n",
        "fields = {\n",
        "    \"name\": str,\n",
        "    \"date\": str,\n",
        "    \"n_speakers\": int,\n",
        "    \"n_participants\": int,\n",
        "    \"price\": str\n",
        "}\n",
        "correct_fields = 0\n",
        "for row in pr_df.itertuples():\n",
        "    parsed_release = parse_press_release(row.pr_text)\n",
        "    parsed_list.append(json.dumps(parsed_release, indent=4))\n",
        "    golden = json.loads(row.pr_parsed)\n",
        "    for field, field_type in fields.items():\n",
        "        golden_field = golden[field]\n",
        "        parsed_field = parsed_release.get(field)\n",
        "        try:\n",
        "            parsed_field = field_type(parsed_field)\n",
        "        except (ValueError, TypeError):\n",
        "            pass\n",
        "        if golden_field == parsed_field:\n",
        "            correct_fields += 1\n",
        "        else:\n",
        "            print(f\"For {golden['name']} {field} {parsed_release.get(field)} doesn't seem the same as {golden[field]}\")\n",
        "\n",
        "print(f\"Correctly extracted {correct_fields} out of {5*len(pr_df)}\")"
      ],
      "metadata": {
        "id": "RSvVEFCWe0DL",
        "outputId": "bd868f65-866c-49ff-ce54-f304024d8929",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For InnovAI Summit 2023 n_speakers 5 doesn't seem the same as 4\n",
            "Correctly extracted 34 out of 35\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Bonus points\n",
        "- Try and compare different ways of establishing the correct answer formatting\n",
        "- Try and compare different LLMs"
      ],
      "metadata": {
        "id": "lcGzo04oe0DL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 2. Localized MMLU\n",
        "\n",
        "Cool thing about structured output, is that it's very easy to make a translated version of a specific dataset, taking into account all the context and outputing in a format, which is super easy to parse. Let's try this on MMLU.\n",
        "\n",
        "**Task:** Write a function which inputs a sample from MMLU and outputs a translated version, using structured outputs.\n",
        "\n",
        "Tip: make sure that the correct answer didn't change."
      ],
      "metadata": {
        "id": "EDQRdBcLe0DL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU datasets"
      ],
      "metadata": {
        "id": "wsZ7lgLXe0DM",
        "outputId": "b77028bd-325b-4761-b55d-921536e411e1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/491.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.2/491.5 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.5/491.5 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/193.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FWXoRpAFC_ZT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List\n",
        "from pydantic import BaseModel\n",
        "\n",
        "class MMLUSample(BaseModel):\n",
        "    ...\n",
        "\n",
        "def translate_mmlu_sample(sample: MMLUSample, target_language: str) -> MMLUSample:\n",
        "    ..."
      ],
      "metadata": {
        "id": "-iMfgJG7e0DM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List\n",
        "from pydantic import BaseModel\n",
        "\n",
        "class MMLUSample(BaseModel):\n",
        "    question: str\n",
        "    A: str\n",
        "    B: str\n",
        "    C: str\n",
        "    D: str\n",
        "    correct_answer: str\n",
        "\n",
        "def translate_mmlu_sample(sample: MMLUSample, target_language: str) -> MMLUSample:\n",
        "    completion = nebius_client.chat.completions.create(\n",
        "        model=llama_model,\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": f\"Translate this MMLU sample into {target_language}\" \\\n",
        "                f\"Question: {sample.question}\\n\" \\\n",
        "                f\"A: {sample.A}\\n\" \\\n",
        "                f\"B: {sample.B}\\n\" \\\n",
        "                f\"C: {sample.C}\\n\" \\\n",
        "                f\"D: {sample.D}\\n\" \\\n",
        "                f\"Correct answer: {sample.correct_answer}\\n\" \\\n",
        "                f\"Translated sample:\"\n",
        "            }\n",
        "        ],\n",
        "        extra_body={\n",
        "            \"guided_json\": MMLUSample.model_json_schema()\n",
        "        },\n",
        "    )\n",
        "\n",
        "    translated = MMLUSample.model_validate_json(completion.choices[0].message.content)\n",
        "    if translated.correct_answer != sample.correct_answer:\n",
        "        translated.correct_answer = sample.correct_answer\n",
        "    return translated"
      ],
      "metadata": {
        "id": "eieWNf6Ze0DM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mmlu_sample = MMLUSample(\n",
        "    question = \"Which of the following statements about Ethernets is typically FALSE?\",\n",
        "    A = \"Ethernets use circuit switching to send messages.\",\n",
        "    B = \"Ethernets use buses with multiple masters.\",\n",
        "    C = \"Ethernet protocols use a collision-detection method to ensure that messages are transmitted properly.\",\n",
        "    D = \"Networks connected by Ethernets are limited in length to a few hundred meters.\",\n",
        "    correct_answer = \"A\"\n",
        ")\n",
        "\n",
        "translate_mmlu_sample(mmlu_sample, target_language=\"German\")"
      ],
      "metadata": {
        "id": "6EqyzfJxe0DM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's remember the code we've written for MMLU evaluator and add a little twist:\n",
        "\n",
        "We'll have both topic and language in which we want to evaluate the model."
      ],
      "metadata": {
        "id": "uwoVgXi7e0DM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets -q"
      ],
      "metadata": {
        "id": "KIB_2RFYe0DM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task**: Modify the following MMLUEvaluator code so that it can also translate the input question and evaluate the performance in a different language."
      ],
      "metadata": {
        "id": "TIqzv_AJe0DM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from typing import List, Dict, Tuple\n",
        "import json\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "from datasets import load_dataset\n",
        "\n",
        "class MMLUEvaluator:\n",
        "    def __init__(self, system_prompt: str = None, prompt: str = None,\n",
        "                 topic: str = \"high_school_mathematics\"):\n",
        "        \"\"\"\n",
        "        Initialize the MMLU evaluator.\n",
        "\n",
        "        Args:\n",
        "            system_prompt: Optional system prompt for the model\n",
        "            prompt: Custom prompt for the model\n",
        "            topic: Which topic to choose\n",
        "        \"\"\"\n",
        "\n",
        "        self.topic = topic\n",
        "        self.topic_prettified = topic.replace(\"_\", \" \")\n",
        "        self.system_prompt = system_prompt or f\"You are an expert in {self.topic_prettified}.\"\n",
        "\n",
        "        self.prompt = \"\"\"You are given a question in {topic_prettified} with four answer options labeled by A, B, C, and D.\n",
        "You need to ponder the question and justify the choice of one of the options A, B, C, or D.\n",
        "At the end, do write the chosen answer option A, B, C, D after #ANSWER:\n",
        "Now, take a deep breath and work out this problem step by step. If you do well, I'll tip you 200$.\n",
        "\n",
        "QUESTION: {question}\n",
        "\n",
        "ANSWER OPTIONS:\n",
        "A: {A}\n",
        "B: {B}\n",
        "C: {C}\n",
        "D: {D}\n",
        "\"\"\"\n",
        "\n",
        "        self.questions, self.choices, self.answers = self.load_mmlu_data(topic=self.topic)\n",
        "\n",
        "    def load_mmlu_data(self, topic: str) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Load MMLU test data on a given topic.\n",
        "\n",
        "        Args:\n",
        "            topic: Which topic to choose\n",
        "\n",
        "        Returns:\n",
        "            DataFrame with questions and answers\n",
        "        \"\"\"\n",
        "\n",
        "        dataset = load_dataset(\"cais/mmlu\", topic, split=\"test\")\n",
        "\n",
        "        dataset = dataset\n",
        "        dataset = pd.DataFrame(dataset)\n",
        "\n",
        "        # Load questions and choices separately\n",
        "        questions = dataset[\"question\"]\n",
        "        choices = pd.DataFrame(\n",
        "            data=dataset[\"choices\"].tolist(), columns=[\"A\", \"B\", \"C\", \"D\"]\n",
        "        )\n",
        "        # In the dataset, true answer labels are in 0-3 format;\n",
        "        # We convert it to A-D\n",
        "        answers = dataset[\"answer\"].map(lambda ans: {0: \"A\", 1: \"B\", 2: \"C\", 3: \"D\"}[ans])\n",
        "\n",
        "        return questions, choices, answers\n",
        "\n",
        "    def extract_answer(self, solution: str) -> str:\n",
        "        \"\"\"\n",
        "        Extract the letter answer from model's response.\n",
        "\n",
        "        Args:\n",
        "            response: Raw model response\n",
        "\n",
        "        Returns:\n",
        "            Extracted answer letter (A, B, C, D, or Failed to parse)\n",
        "        \"\"\"\n",
        "        # Look for a single letter answer in the response\n",
        "        try:\n",
        "            answer = solution.split('#ANSWER:')[1].strip()\n",
        "        except:\n",
        "            answer = \"Failed to parse\"\n",
        "        return answer\n",
        "\n",
        "    def evaluate_single_question(self, question: str, choices: Dict[str, str],\n",
        "                                 correct_answer: str,\n",
        "                                 client, model) -> Tuple[bool, str]:\n",
        "        \"\"\"\n",
        "        Evaluate a single question.\n",
        "\n",
        "        Args:\n",
        "            question: Formatted question string\n",
        "            correct_answer: Correct answer letter\n",
        "\n",
        "        Returns:\n",
        "            Tuple of (is_correct, extracted_answer, model_response)\n",
        "        \"\"\"\n",
        "        try:\n",
        "            model_response = answer_with_llm(\n",
        "                prompt=self.prompt.format(\n",
        "                    client=client, model=model,\n",
        "                    topic_prettified=self.topic_prettified,\n",
        "                    question=question,\n",
        "                    A=choices['A'], B=choices['B'], C=choices['C'], D=choices['D']\n",
        "                ),\n",
        "                system_prompt=self.system_prompt,\n",
        "                prettify=False\n",
        "            )\n",
        "            answer = self.extract_answer(model_response)\n",
        "            is_correct = (answer.upper() == correct_answer.upper())\n",
        "            return is_correct, answer, model_response\n",
        "        except Exception as e:\n",
        "            print(f\"Error evaluating question: {e}\")\n",
        "            return False, None, None\n",
        "\n",
        "    def run_evaluation(self, client=nebius_client, model=\"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n",
        "                       n_questions=50) -> Dict:\n",
        "        \"\"\"\n",
        "        Run evaluation of a given model on the first n_questions.\n",
        "\n",
        "        Args:\n",
        "            client: Which client to use (OpenAI or Nebius)\n",
        "            model: Which model to use\n",
        "            n_questions: How many first questions to take\n",
        "\n",
        "        Returns:\n",
        "            Dictionary with evaluation metrics\n",
        "        \"\"\"\n",
        "        evaluation_log = []\n",
        "        correct_count = 0\n",
        "\n",
        "        if n_questions:\n",
        "            n_questions = min(n_questions, len(self.questions))\n",
        "        else:\n",
        "            n_questions = len(self.questions)\n",
        "\n",
        "        for i in tqdm(range(n_questions)):\n",
        "            is_correct, answer, model_response = self.evaluate_single_question(\n",
        "                question=self.questions[i],\n",
        "                choices=self.choices.iloc[i],\n",
        "                correct_answer=self.answers[i],\n",
        "                client=client,\n",
        "                model=model,\n",
        "            )\n",
        "\n",
        "            if is_correct:\n",
        "                correct_count += 1\n",
        "\n",
        "            evaluation_log.append({\n",
        "                'answer': answer,\n",
        "                'model_response': model_response,\n",
        "                'is_correct': is_correct\n",
        "            })\n",
        "\n",
        "        accuracy = correct_count / n_questions\n",
        "        evaluation_results = {\n",
        "            'accuracy': accuracy,\n",
        "            'evaluation_log': evaluation_log\n",
        "        }\n",
        "\n",
        "        return evaluation_results\n"
      ],
      "metadata": {
        "id": "KmbtCJnNe0DM"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from typing import List, Dict, Tuple\n",
        "import json\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "from datasets import load_dataset\n",
        "\n",
        "class MMLUEvaluator:\n",
        "    def __init__(self, system_prompt: str = None, prompt: str = None,\n",
        "                 topic: str = \"high_school_mathematics\",\n",
        "                 language: str = \"English\"):\n",
        "        \"\"\"\n",
        "        Initialize the MMLU evaluator.\n",
        "\n",
        "        Args:\n",
        "            system_prompt: Optional system prompt for the model\n",
        "            prompt: Custom prompt for the model\n",
        "            topic: Which topic to choose\n",
        "        \"\"\"\n",
        "\n",
        "        self.topic = topic\n",
        "        self.language = language\n",
        "        self.topic_prettified = topic.replace(\"_\", \" \")\n",
        "        self.system_prompt = system_prompt or f\"You are an expert in {self.topic_prettified}.\"\n",
        "\n",
        "        self.prompt = \"\"\"You are given a question in {topic_prettified} with four answer options labeled by A, B, C, and D.\n",
        "You need to ponder the question and justify the choice of one of the options A, B, C, or D.\n",
        "At the end, do write the chosen answer option A, B, C, D after #ANSWER:\n",
        "Now, take a deep breath and work out this problem step by step. If you do well, I'll tip you 200$.\n",
        "\n",
        "QUESTION: {question}\n",
        "\n",
        "ANSWER OPTIONS:\n",
        "A: {A}\n",
        "B: {B}\n",
        "C: {C}\n",
        "D: {D}\n",
        "\"\"\"\n",
        "\n",
        "        self.questions, self.choices, self.answers = self.load_mmlu_data(topic=self.topic)\n",
        "\n",
        "    def load_mmlu_data(self, topic: str) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Load MMLU test data on a given topic.\n",
        "\n",
        "        Args:\n",
        "            topic: Which topic to choose\n",
        "\n",
        "        Returns:\n",
        "            DataFrame with questions and answers\n",
        "        \"\"\"\n",
        "\n",
        "        dataset = load_dataset(\"cais/mmlu\", topic, split=\"test\")\n",
        "\n",
        "        dataset = dataset\n",
        "        dataset = pd.DataFrame(dataset)\n",
        "\n",
        "        # Load questions and choices separately\n",
        "        questions = dataset[\"question\"]\n",
        "        choices = pd.DataFrame(\n",
        "            data=dataset[\"choices\"].tolist(), columns=[\"A\", \"B\", \"C\", \"D\"]\n",
        "        )\n",
        "        # In the dataset, true answer labels are in 0-3 format;\n",
        "        # We convert it to A-D\n",
        "        answers = dataset[\"answer\"].map(lambda ans: {0: \"A\", 1: \"B\", 2: \"C\", 3: \"D\"}[ans])\n",
        "\n",
        "        return questions, choices, answers\n",
        "\n",
        "    def extract_answer(self, solution: str) -> str:\n",
        "        \"\"\"\n",
        "        Extract the letter answer from model's response.\n",
        "\n",
        "        Args:\n",
        "            response: Raw model response\n",
        "\n",
        "        Returns:\n",
        "            Extracted answer letter (A, B, C, D, or Failed to parse)\n",
        "        \"\"\"\n",
        "        # Look for a single letter answer in the response\n",
        "        try:\n",
        "            answer = solution.split('#ANSWER:')[1].strip()\n",
        "        except:\n",
        "            answer = \"Failed to parse\"\n",
        "        return answer\n",
        "\n",
        "    def evaluate_single_question(self, question: str, choices: Dict[str, str],\n",
        "                                 correct_answer: str,\n",
        "                                 client, model) -> Tuple[bool, str]:\n",
        "        \"\"\"\n",
        "        Evaluate a single question.\n",
        "\n",
        "        Args:\n",
        "            question: Formatted question string\n",
        "            correct_answer: Correct answer letter\n",
        "\n",
        "        Returns:\n",
        "            Tuple of (is_correct, extracted_answer, model_response)\n",
        "        \"\"\"\n",
        "        try:\n",
        "            if self.language != \"English\":\n",
        "                sample = MMLUSample(\n",
        "                    question=question,\n",
        "                    A=choices['A'], B=choices['B'], C=choices['C'], D=choices['D'],\n",
        "                    correct_answer=correct_answer\n",
        "                )\n",
        "                translated = translate_mmlu_sample(sample, target_language=self.language)\n",
        "                question = translated.question\n",
        "                choices = {\"A\": translated.A, \"B\": translated.B, \"C\": translated.C, \"D\": translated.D}\n",
        "                correct_answer = translated.correct_answer\n",
        "            model_response = answer_with_llm(\n",
        "                prompt=self.prompt.format(\n",
        "                    client=client, model=model,\n",
        "                    topic_prettified=self.topic_prettified,\n",
        "                    question=question,\n",
        "                    A=choices['A'], B=choices['B'], C=choices['C'], D=choices['D']\n",
        "                ),\n",
        "                system_prompt=self.system_prompt,\n",
        "                prettify=False\n",
        "            )\n",
        "            answer = self.extract_answer(model_response)\n",
        "            is_correct = (answer.upper() == correct_answer.upper())\n",
        "            return is_correct, answer, model_response\n",
        "        except Exception as e:\n",
        "            print(f\"Error evaluating question: {e}\")\n",
        "            return False, None, None\n",
        "\n",
        "    def run_evaluation(self, client=nebius_client, model=\"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n",
        "                       n_questions=50) -> Dict:\n",
        "        \"\"\"\n",
        "        Run evaluation of a given model on the first n_questions.\n",
        "\n",
        "        Args:\n",
        "            client: Which client to use (OpenAI or Nebius)\n",
        "            model: Which model to use\n",
        "            n_questions: How many first questions to take\n",
        "\n",
        "        Returns:\n",
        "            Dictionary with evaluation metrics\n",
        "        \"\"\"\n",
        "        evaluation_log = []\n",
        "        correct_count = 0\n",
        "\n",
        "        if n_questions:\n",
        "            n_questions = min(n_questions, len(self.questions))\n",
        "        else:\n",
        "            n_questions = len(self.questions)\n",
        "\n",
        "        for i in tqdm(range(n_questions)):\n",
        "            is_correct, answer, model_response = self.evaluate_single_question(\n",
        "                question=self.questions[i],\n",
        "                choices=self.choices.iloc[i],\n",
        "                correct_answer=self.answers[i],\n",
        "                client=client,\n",
        "                model=model,\n",
        "            )\n",
        "\n",
        "            if is_correct:\n",
        "                correct_count += 1\n",
        "\n",
        "            evaluation_log.append({\n",
        "                'answer': answer,\n",
        "                'model_response': model_response,\n",
        "                'is_correct': is_correct\n",
        "            })\n",
        "\n",
        "        accuracy = correct_count / n_questions\n",
        "        evaluation_results = {\n",
        "            'accuracy': accuracy,\n",
        "            'evaluation_log': evaluation_log\n",
        "        }\n",
        "\n",
        "        return evaluation_results"
      ],
      "metadata": {
        "id": "-CI17O6-DP50"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testing"
      ],
      "metadata": {
        "id": "78Eqh6jhe0DM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "evaluator = MMLUEvaluator(topic=\"medical_genetics\", language=\"English\")\n",
        "\n",
        "results = evaluator.run_evaluation(model=\"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n",
        "                         n_questions=50)\n",
        "print(f'\\nAccuracy: {results[\"accuracy\"]}')"
      ],
      "metadata": {
        "id": "DpQ9Lp6_e0DM",
        "outputId": "19d9a5c1-3986-4456-ffa4-15bd4561d733",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 50/50 [09:46<00:00, 11.74s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Accuracy: 0.86\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "evaluator_de = MMLUEvaluator(topic=\"medical_genetics\", language=\"German\")\n",
        "\n",
        "results_de = evaluator_de.run_evaluation(model=\"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n",
        "                         n_questions=10)\n",
        "print(f'\\nAccuracy: {results_de[\"accuracy\"]}')"
      ],
      "metadata": {
        "id": "-_jrw-z0e0DN",
        "outputId": "9cbfd134-b038-43fb-8ca3-7dc313fa3062",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [00:00<00:00, 13999.68it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error evaluating question: name 'MMLUSample' is not defined\n",
            "Error evaluating question: name 'MMLUSample' is not defined\n",
            "Error evaluating question: name 'MMLUSample' is not defined\n",
            "Error evaluating question: name 'MMLUSample' is not defined\n",
            "Error evaluating question: name 'MMLUSample' is not defined\n",
            "Error evaluating question: name 'MMLUSample' is not defined\n",
            "Error evaluating question: name 'MMLUSample' is not defined\n",
            "Error evaluating question: name 'MMLUSample' is not defined\n",
            "Error evaluating question: name 'MMLUSample' is not defined\n",
            "Error evaluating question: name 'MMLUSample' is not defined\n",
            "\n",
            "Accuracy: 0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FIW2nMg8e0DN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}