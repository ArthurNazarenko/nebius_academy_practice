{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ArthurNazarenko/nebius_academy_practice/blob/main/topic3/3.1_the_concept_of_rag_solutions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LLMOps Essentials 3.1. The concept of RAG"
      ],
      "metadata": {
        "id": "Vm506vpf9u9b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Practice solutions"
      ],
      "metadata": {
        "id": "_Vk0MnZc6WHG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 1: Retrieval as a Tool\n",
        "\n",
        "In the example above, we built a bot that uses retrieval at every step — but that approach isn't always appropriate. In this task, you'll turn the bot into an agent that calls retrieval only when the LLM deems it's necessary.\n",
        "\n",
        "Compared with the previous RAG chatbot, this version will be more flexible, avoiding awkward responses to messages like “Hi there!” that don't benefit from retrieval at all.\n",
        "\n",
        "You're free to design your own architecture, of course, but we suggest combining ideas from both `ChatBotWithRAG` and `NPCTraderAgent` in the agent notebook. If you like, you can set up the decision to call retrieval via a classifier LLM call — similar to how intent classification was used for trade in `NPCTraderAgent`. But for now, we recommend simply using native LLM tool calling."
      ],
      "metadata": {
        "id": "f1hL75N53sQM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Solution**"
      ],
      "metadata": {
        "id": "fk26C6FS6hlc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q openai tavily-python"
      ],
      "metadata": {
        "id": "45VAkPvtfr4b"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "tavily_api_key=userdata.get('tavily_api_key')\n",
        "nebius_api_key=userdata.get('nebius_api_key')\n",
        "\n",
        "os.environ[\"TAVILY_API_KEY\"] = tavily_api_key\n",
        "os.environ[\"NEBIUS_API_KEY\"] = nebius_api_key"
      ],
      "metadata": {
        "id": "8zbPQZdzfysq"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Basically we reused the design pattern we used for the NPC Trader Agent from Topic 2. But this time we only have one tool, which is web search."
      ],
      "metadata": {
        "id": "OylEWYzM6j2S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict, deque\n",
        "from openai import OpenAI\n",
        "from typing import Dict, Any, List, Optional, Callable\n",
        "import json\n",
        "import traceback\n",
        "\n",
        "class RAGAgent:\n",
        "    def __init__(self, client: OpenAI, model: str, search_client,\n",
        "                 history_size: int = 10,\n",
        "                 get_system_message: Callable[[], Optional[Dict[str, str]]] = None,\n",
        "                 search_depth: str = \"advanced\",\n",
        "                 max_search_results: int = 5\n",
        "                 ):\n",
        "        \"\"\"Initialize the chat agent with RAG tool.\n",
        "\n",
        "        Args:\n",
        "            client: OpenAI client instance\n",
        "            model: The model to use (e.g., \"gpt-4o-mini\")\n",
        "            search_client: Search client instance (for example, Tavily)\n",
        "            history_size: Number of messages to keep in history per user\n",
        "            get_system_message: Function to retrieve the system message\n",
        "            search_depth: Depth of web search ('basic' or 'advanced')\n",
        "            max_search_results: Maximum number of search results to retrieve\n",
        "        \"\"\"\n",
        "        self.client = client\n",
        "        self.model = model\n",
        "        self.search_client = search_client\n",
        "        self.history_size = history_size\n",
        "\n",
        "        # If no system message function is provided, use the default one\n",
        "        self.get_system_message = get_system_message if get_system_message else self._default_system_message\n",
        "\n",
        "        self.search_depth = search_depth\n",
        "        self.max_search_results = max_search_results\n",
        "\n",
        "        # Initialize chat history storage\n",
        "        self.chat_histories = defaultdict(lambda: deque(maxlen=history_size))\n",
        "\n",
        "        # Define the tools available to the model\n",
        "        self.tools = [\n",
        "            {\n",
        "                \"type\": \"function\",\n",
        "                \"function\": {\n",
        "                    \"name\": \"retrieve_information\",\n",
        "                    \"description\": \"Search the web for information relevant to the user's query when you need additional context to provide a complete and accurate answer.\",\n",
        "                    \"parameters\": {\n",
        "                        \"type\": \"object\",\n",
        "                        \"properties\": {\n",
        "                            \"query\": {\n",
        "                                \"type\": \"string\",\n",
        "                                \"description\": \"The search query to use for retrieving information. This should be a refined version of the user's question optimized for web search.\"\n",
        "                            }\n",
        "                        },\n",
        "                        \"required\": [\"query\"]\n",
        "                    }\n",
        "                }\n",
        "            }\n",
        "        ]\n",
        "\n",
        "        # Map available tool functions\n",
        "        self.available_tools = {\n",
        "            \"retrieve_information\": self.retrieve_information\n",
        "        }\n",
        "\n",
        "    def _default_system_message(self) -> Dict[str, str]:\n",
        "        \"\"\"Default system message if none is provided.\"\"\"\n",
        "        return {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": \"\"\"You are a helpful assistant.\"\"\"\n",
        "        }\n",
        "\n",
        "    def retrieve_information(self, query: str) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Perform a web search using the search client and format the results.\n",
        "\n",
        "        Args:\n",
        "            query: The search query\n",
        "\n",
        "        Returns:\n",
        "            Dictionary containing formatted search results and metadata\n",
        "        \"\"\"\n",
        "        try:\n",
        "            search_results = self.search_client.search(\n",
        "                query=query,\n",
        "                search_depth=self.search_depth,\n",
        "                max_results=self.max_search_results\n",
        "            )\n",
        "\n",
        "            formatted_results = []\n",
        "            for result in search_results.get('results', []):\n",
        "                content = result.get('content', '').strip()\n",
        "                url = result.get('url', '')\n",
        "                if content:\n",
        "                    formatted_results.append(f\"Content: {content}\\nSource: {url}\\n\")\n",
        "\n",
        "            # Join all results with proper formatting\n",
        "            context = \"\\n\".join(formatted_results)\n",
        "\n",
        "            return {\n",
        "                \"context\": context,\n",
        "                \"query\": query,\n",
        "                \"num_results\": len(formatted_results),\n",
        "                \"success\": True,\n",
        "                \"message\": f\"Retrieved {len(formatted_results)} results for query: '{query}'\"\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in retrieve_information: {str(e)}\")\n",
        "            return {\n",
        "                \"context\": \"\",\n",
        "                \"query\": query,\n",
        "                \"num_results\": 0,\n",
        "                \"success\": False,\n",
        "                \"message\": f\"Failed to retrieve information: {str(e)}\"\n",
        "            }\n",
        "\n",
        "    def process_tool_calls(self, tool_calls, user_id: str, debug: bool = False) -> List[Dict[str, Any]]:\n",
        "        \"\"\"Process tool calls from the LLM response.\"\"\"\n",
        "        tool_responses = []\n",
        "\n",
        "        for tool_call in tool_calls:\n",
        "            function_name = tool_call.function.name\n",
        "            function_id = tool_call.id\n",
        "\n",
        "            try:\n",
        "                function_args = json.loads(tool_call.function.arguments)\n",
        "            except Exception as e:\n",
        "                print(f\"Error parsing arguments: {e}\")\n",
        "                function_args = {}\n",
        "\n",
        "            if debug:\n",
        "                print(f\"#Processing tool call:\\n {function_name}, args: {function_args}\\n\")\n",
        "\n",
        "            if function_name not in self.available_tools:\n",
        "                print(f\"Unknown function: {function_name}\")\n",
        "                continue\n",
        "\n",
        "            # Get the function to call\n",
        "            tool_function = self.available_tools[function_name]\n",
        "\n",
        "            try:\n",
        "                # Execute the function\n",
        "                result = tool_function(**function_args)\n",
        "\n",
        "                # Format the result specifically for retrieval\n",
        "                if function_name == \"retrieve_information\":\n",
        "                    # Format with <context> tags as requested\n",
        "                    content = json.dumps({\n",
        "                        \"result\": {\n",
        "                            \"message\": result[\"message\"],\n",
        "                            \"formatted_context\": f\"<context>\\n{result['context']}\\n</context>\"\n",
        "                        }\n",
        "                    })\n",
        "                else:\n",
        "                    # Generic handling for any future tools\n",
        "                    content = json.dumps(result)\n",
        "\n",
        "                if debug:\n",
        "                    print(f\"#Tool result:\\n{content[:200]}...\\n\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error executing {function_name}: {e}\")\n",
        "                print(traceback.format_exc())\n",
        "                content = json.dumps({\"error\": str(e)})\n",
        "\n",
        "            # Create the tool response\n",
        "            tool_responses.append({\n",
        "                \"tool_call_id\": function_id,\n",
        "                \"role\": \"tool\",\n",
        "                \"name\": function_name,\n",
        "                \"content\": content\n",
        "            })\n",
        "\n",
        "        return tool_responses\n",
        "\n",
        "    def chat(self, user_message: str, user_id: str, debug: bool = False) -> str:\n",
        "        \"\"\"Process a user message and return the agent's response.\n",
        "\n",
        "        Args:\n",
        "            user_message: The message from the user\n",
        "            user_id: Unique identifier for the user\n",
        "            debug: Whether to print debug information\n",
        "\n",
        "        Returns:\n",
        "            str: The agent's response\n",
        "        \"\"\"\n",
        "        # Add new user message to history\n",
        "        user_message_dict = {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": user_message\n",
        "        }\n",
        "        self.chat_histories[user_id].append(user_message_dict)\n",
        "\n",
        "        # Construct messages for the LLM\n",
        "        messages = []\n",
        "        system_message = self.get_system_message()\n",
        "        if system_message:\n",
        "            messages.append(system_message)\n",
        "\n",
        "        # Add conversation history\n",
        "        history = list(self.chat_histories[user_id])\n",
        "        if history:\n",
        "            messages.extend(history)\n",
        "\n",
        "        try:\n",
        "            # First API call that might use tools\n",
        "            completion = self.client.chat.completions.create(\n",
        "                model=self.model,\n",
        "                messages=messages,\n",
        "                temperature=0.7,\n",
        "                tools=self.tools,\n",
        "                tool_choice=\"auto\"\n",
        "            )\n",
        "\n",
        "            if debug:\n",
        "                print(f\"#First completion response:\\n{completion}\\n\")\n",
        "\n",
        "            # Get the assistant's response\n",
        "            assistant_message = completion.choices[0].message\n",
        "            response_content = assistant_message.content or \"\"\n",
        "\n",
        "            # Check for tool calls\n",
        "            tool_calls = getattr(assistant_message, 'tool_calls', None)\n",
        "\n",
        "            # If there are tool calls, process them\n",
        "            if tool_calls:\n",
        "                if debug:\n",
        "                    print(f\"#Tool calls detected: {len(tool_calls)}\\n\")\n",
        "\n",
        "                # Add the assistant's message to history for proper conversation tracking\n",
        "                messages.append({\n",
        "                    \"role\": \"assistant\",\n",
        "                    \"content\": response_content,\n",
        "                    \"tool_calls\": [\n",
        "                        {\n",
        "                            \"id\": tc.id,\n",
        "                            \"type\": \"function\",\n",
        "                            \"function\": {\n",
        "                                \"name\": tc.function.name,\n",
        "                                \"arguments\": tc.function.arguments\n",
        "                            }\n",
        "                        } for tc in tool_calls\n",
        "                    ]\n",
        "                })\n",
        "\n",
        "                # Process tool calls and get responses\n",
        "                tool_responses = self.process_tool_calls(tool_calls, user_id, debug=debug)\n",
        "\n",
        "                # Add tool responses to messages\n",
        "                for tool_response in tool_responses:\n",
        "                    messages.append(tool_response)\n",
        "\n",
        "                # Make a second call to get the final response with retrieved information\n",
        "                second_completion = self.client.chat.completions.create(\n",
        "                    model=self.model,\n",
        "                    messages=messages,\n",
        "                    temperature=0.7\n",
        "                )\n",
        "\n",
        "                # Use the final response that includes tool results\n",
        "                response_content = second_completion.choices[0].message.content or \"\"\n",
        "\n",
        "                if debug:\n",
        "                    print(f\"#Final response after tool calls:\\n{response_content[:100]}...\\n\")\n",
        "\n",
        "            # Store the final response in history\n",
        "            self.chat_histories[user_id].append({\n",
        "                \"role\": \"assistant\",\n",
        "                \"content\": response_content\n",
        "            })\n",
        "\n",
        "            return response_content\n",
        "\n",
        "        except Exception as e:\n",
        "            error_msg = f\"Error in chat: {str(e)}\"\n",
        "            print(error_msg)\n",
        "            print(traceback.format_exc())\n",
        "            return error_msg\n",
        "\n",
        "    def get_chat_history(self, user_id: str) -> list:\n",
        "        \"\"\"Retrieve the chat history for a specific user.\n",
        "\n",
        "        Args:\n",
        "            user_id: Unique identifier for the user\n",
        "\n",
        "        Returns:\n",
        "            list: List of message dictionaries\n",
        "        \"\"\"\n",
        "        return list(self.chat_histories[user_id])\n"
      ],
      "metadata": {
        "id": "Nzz99NwPaz5w"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "from tavily import TavilyClient\n",
        "import os\n",
        "\n",
        "nebius_client = OpenAI(\n",
        "    base_url=\"https://api.studio.nebius.ai/v1/\",\n",
        "    api_key=os.environ.get(\"NEBIUS_API_KEY\"),\n",
        ")\n",
        "\n",
        "tavily_client = TavilyClient(api_key=os.environ.get(\"TAVILY_API_KEY\"))\n",
        "\n",
        "model = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
        "\n",
        "rag_agent = RAGAgent(client=nebius_client, model=model, search_client=tavily_client)"
      ],
      "metadata": {
        "id": "ItseRK2LfqjD"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's try asking the agent different questions to check how much it relies on retrieval and in which situations it will answer on its own."
      ],
      "metadata": {
        "id": "dRxiVqB46xis"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate a user ID\n",
        "import uuid\n",
        "user_id = str(uuid.uuid4())\n",
        "print(f\"Demo User ID: {user_id}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kYABknVrgyHA",
        "outputId": "11730a7b-8539-4d6e-fe5d-f649cf740264"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Demo User ID: 1a16c9fc-8f65-44b4-88fe-0302d45d5fcb\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result = rag_agent.chat(user_message=\"Hi there!\", user_id=user_id, debug=True)\n",
        "print(f\"=====\\n{result}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TavayVUigguY",
        "outputId": "ade4fdcb-5acb-460a-e71d-a27d36b4c88c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#First completion response:\n",
            "ChatCompletion(id='chatcmpl-bdd3bdace2e64d789fd7115bfac57a94', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='Hello! How can I assist you today?', refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[], reasoning_content=None), stop_reason=None)], created=1750238481, model='meta-llama/Meta-Llama-3.1-8B-Instruct', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=10, prompt_tokens=227, total_tokens=237, completion_tokens_details=None, prompt_tokens_details=None), prompt_logprobs=None)\n",
            "\n",
            "=====\n",
            "Hello! How can I assist you today?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result = rag_agent.chat(user_message=\"Tell me about the Penitent One from videogame Blasphemous\",\n",
        "                        user_id=user_id, debug=True)\n",
        "print(f\"=====\\n{result}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dXXZmZLXhDql",
        "outputId": "543bdd2a-1949-4f1c-cfb7-5252c0229a1e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#First completion response:\n",
            "ChatCompletion(id='chatcmpl-b6873b42081b4ef7b357e6f28be7a1eb', choices=[Choice(finish_reason='tool_calls', index=0, logprobs=None, message=ChatCompletionMessage(content=None, refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='chatcmpl-tool-0f52abc5bfac4caf8fa61a2665a8f98c', function=Function(arguments='{\"query\": \"Blasphemous Penitent One\"}', name='retrieve_information'), type='function')], reasoning_content=None), stop_reason=128008)], created=1750238568, model='meta-llama/Meta-Llama-3.1-8B-Instruct', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=24, prompt_tokens=261, total_tokens=285, completion_tokens_details=None, prompt_tokens_details=None), prompt_logprobs=None)\n",
            "\n",
            "#Tool calls detected: 1\n",
            "\n",
            "#Processing tool call:\n",
            " retrieve_information, args: {'query': 'Blasphemous Penitent One'}\n",
            "\n",
            "#Tool result:\n",
            "{\"result\": {\"message\": \"Retrieved 5 results for query: 'Blasphemous Penitent One'\", \"formatted_context\": \"<context>\\nContent: **The Penitent One** is the\\u00a0playable character in **[Blasphemous](/Bl...\n",
            "\n",
            "#Final response after tool calls:\n",
            "The Penitent One is the playable character in the videogame Blasphemous. He is a rogue nomad and the...\n",
            "\n",
            "=====\n",
            "The Penitent One is the playable character in the videogame Blasphemous. He is a rogue nomad and the last survivor of the massacre of the \"Silent Sorrow.\" The Penitent One has taken a vow of silence as his penance, so he does not speak. He wears silver-plated armor and wields a unique sword called the Mea Culpa. Throughout the game, the player controls The Penitent One as he navigates through the world of Cvstodia, fighting enemies and bosses, and discovering items and abilities that aid him in his journey.\n",
            "\n",
            "The Penitent One's story is deeply connected to the game's lore, and his journey is a reflection of the world's struggle against the Miracle, a terrible fate that has fallen upon Cvstodia. The player must guide The Penitent One through his journey, overcoming challenges and making choices that impact the game's story and outcome.\n",
            "\n",
            "Overall, The Penitent One is a complex and intriguing character, and his story serves as the core of the Blasphemous videogame experience.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result = rag_agent.chat(user_message=\"Who is Crisanta from this game?\",\n",
        "                        user_id=user_id, debug=True)\n",
        "print(f\"=====\\n{result}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Un3bJCFNhlYv",
        "outputId": "ee20e449-e4ff-4982-8005-94f7cc6a1dd0"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#First completion response:\n",
            "ChatCompletion(id='chatcmpl-c2f9d1edc4814a05a8519237174a28c8', choices=[Choice(finish_reason='tool_calls', index=0, logprobs=None, message=ChatCompletionMessage(content=None, refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='chatcmpl-tool-73b38b0720e74eb29b129d26213e2cd8', function=Function(arguments='{\"query\": \"Crisanta, Blasphemous game character\"}', name='retrieve_information'), type='function')], reasoning_content=None), stop_reason=128008)], created=1750238748, model='meta-llama/Meta-Llama-3.1-8B-Instruct', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=26, prompt_tokens=501, total_tokens=527, completion_tokens_details=None, prompt_tokens_details=None), prompt_logprobs=None)\n",
            "\n",
            "#Tool calls detected: 1\n",
            "\n",
            "#Processing tool call:\n",
            " retrieve_information, args: {'query': 'Crisanta, Blasphemous game character'}\n",
            "\n",
            "#Tool result:\n",
            "{\"result\": {\"message\": \"Retrieved 5 results for query: 'Crisanta, Blasphemous game character'\", \"formatted_context\": \"<context>\\nContent: **Crisanta of the Wrapped Agony** is a knight devoted to [His ...\n",
            "\n",
            "#Final response after tool calls:\n",
            "Crisanta is a character from the videogame Blasphemous. She is a knight devoted to His Holiness Escr...\n",
            "\n",
            "=====\n",
            "Crisanta is a character from the videogame Blasphemous. She is a knight devoted to His Holiness Escribar and is one of the last bosses in the game. Crisanta is a penitent who has taken a vow of silence, just like The Penitent One, but her penance is different and opposite to his. She believes that their penances are complementary and has participated in the destruction of the Brotherhood of the Silent Sorrow. Crisanta is the captain of the Anointed Legion and is a powerful foe in the game.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result = rag_agent.chat(user_message=\"What will happen to Guts from the Berserk manga?\",\n",
        "                        user_id=user_id, debug=True)\n",
        "print(f\"=====\\n{result}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l-U_ogmxh6id",
        "outputId": "dc8fa477-ac44-4429-b2f9-85f7e13fc712"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#First completion response:\n",
            "ChatCompletion(id='chatcmpl-32529a4cb7ed4b41948ba26cd4b9c53e', choices=[Choice(finish_reason='tool_calls', index=0, logprobs=None, message=ChatCompletionMessage(content=None, refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='chatcmpl-tool-c12872abe634422187337aa4938bb5ae', function=Function(arguments='{\"query\": \"Guts Berserk manga ending\"}', name='retrieve_information'), type='function')], reasoning_content=None), stop_reason=128008)], created=1750238846, model='meta-llama/Meta-Llama-3.1-8B-Instruct', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=22, prompt_tokens=639, total_tokens=661, completion_tokens_details=None, prompt_tokens_details=None), prompt_logprobs=None)\n",
            "\n",
            "#Tool calls detected: 1\n",
            "\n",
            "#Processing tool call:\n",
            " retrieve_information, args: {'query': 'Guts Berserk manga ending'}\n",
            "\n",
            "#Tool result:\n",
            "{\"result\": {\"message\": \"Retrieved 5 results for query: 'Guts Berserk manga ending'\", \"formatted_context\": \"<context>\\nContent: Before any predictions about *Beserk*'s future installments can be made, ...\n",
            "\n",
            "#Final response after tool calls:\n",
            "The ending of the Berserk manga is still uncertain, as it was left unfinished by the creator, Kentar...\n",
            "\n",
            "=====\n",
            "The ending of the Berserk manga is still uncertain, as it was left unfinished by the creator, Kentaro Miura, after his passing in 2021. However, it was officially announced by the publisher, Hakusensha, that the story will continue with a new artist, Koji Mori, taking over.\n",
            "\n",
            "As for Guts' fate, it's still unknown, but based on the narrative, one possible scenario is that he defeats Griffith. However, this is just speculation, and the actual ending will depend on the new artist's interpretation and continuation of the story.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result = rag_agent.chat(user_message=\"How much is 17.5 + 290.0?\",\n",
        "                        user_id=user_id, debug=True)\n",
        "print(f\"=====\\n{result}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6I0avV_2iRKy",
        "outputId": "2a517fc7-0a3c-45e7-9168-a2f4e6952daf"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#First completion response:\n",
            "ChatCompletion(id='chatcmpl-09ffaf93b0c749fc961531ae58dcadac', choices=[Choice(finish_reason='tool_calls', index=0, logprobs=None, message=ChatCompletionMessage(content=None, refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='chatcmpl-tool-10b2c243a6a84b0da1c22cdbbd8d1a75', function=Function(arguments='{\"query\": \"17.5 + 290.0\"}', name='retrieve_information'), type='function')], reasoning_content=None), stop_reason=128008)], created=1750238913, model='meta-llama/Meta-Llama-3.1-8B-Instruct', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=24, prompt_tokens=798, total_tokens=822, completion_tokens_details=None, prompt_tokens_details=None), prompt_logprobs=None)\n",
            "\n",
            "#Tool calls detected: 1\n",
            "\n",
            "#Processing tool call:\n",
            " retrieve_information, args: {'query': '17.5 + 290.0'}\n",
            "\n",
            "#Tool result:\n",
            "{\"result\": {\"message\": \"Retrieved 5 results for query: '17.5 + 290.0'\", \"formatted_context\": \"<context>\\nContent: 9.3, 4.2. 5-Mar-2008, -17.5, 62.0, 290.0, 24.1, 0.0. 6-Mar-2008, -21.8, 55.0, 280.0, 1...\n",
            "\n",
            "#Final response after tool calls:\n",
            "To answer your question, 17.5 + 290.0 is 307.5....\n",
            "\n",
            "=====\n",
            "To answer your question, 17.5 + 290.0 is 307.5.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Probably, our agent over-relies on retrieval. I'm pretty sure that Llama 3.1 knows well who Yann LeCun is. But this could be tuned with more accurate prompting. However, general conversation topics such as \"what's the weather today\" are answered without web search."
      ],
      "metadata": {
        "id": "_4aHT7h4iCL9"
      }
    }
  ]
}