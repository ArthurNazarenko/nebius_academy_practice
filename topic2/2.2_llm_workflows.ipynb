{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ArthurNazarenko/nebius_academy_practice/blob/main/topic2/2.2_llm_workflows.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a2Adag0lHGp5"
      },
      "source": [
        "# LLM Engineering Essentials by Nebius Academy\n",
        "\n",
        "Course github: [link](https://github.com/Nebius-Academy/LLM-Engineering-Essentials/tree/main)\n",
        "\n",
        "Author: Alex Umnov\n",
        "\n",
        "Links:\n",
        "- [LinkedIn](www.linkedin.com/in/alex-umnov)\n",
        "- Discord Profile: *alexumnov* , best to tag at #nebius-academy\n",
        "\n",
        "The course is in development now, with more materials coming soon. [Subscribe to stay updated](https://academy.nebius.com/llm-engineering-essentials/update/)\n",
        "\n",
        "# 2.2 LLM Workflows\n",
        "\n",
        "In Topic 1 we mostly studied how to get value from one LLM working in a single-call or a chat mode. But it's only a beginning! So much more may be achieved by orchestrating a complex workflow combining several LLM calls, tools, etc.\n",
        "\n",
        "Orchestration will be the core idea of Topic 2. We'll guide you through:\n",
        "\n",
        "* **LLM workflows** - **manual orchestration of several LLM calls inside one system** - in this notebook\n",
        "* Orchestrated and native LLM reasoning processes in notebooks **2.3-5**\n",
        "* Native tool usage and LLM agent basics in **2.6**\n",
        "* LLM-powered planning and agentic systems in **2.7**\n",
        "\n",
        "So, let's start this exciting journey!\n",
        "\n",
        "In this notebooks, we'll discuss how to combine LLM calls in meaningful and flexible ways. We'll mostly follow the [Building Effective Agents](https://www.anthropic.com/engineering/building-effective-agents) article by Anthropic in its workflow classification - and we really recommend you to browse through it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JFWGxepmhVLh"
      },
      "source": [
        "## Getting things ready"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "m3UBYmNgE90I",
        "outputId": "14424a88-1de0-44aa-fa0e-dcacf4b3cdce",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/730.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m409.6/730.2 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m730.2/730.2 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install openai -qU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "GeFeBF0mKYVL"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "from openai import OpenAI\n",
        "import os\n",
        "\n",
        "os.environ['NEBIUS_API_KEY'] = userdata.get(\"nebius_api_key\")\n",
        "\n",
        "nebius_client = OpenAI(\n",
        "    base_url=\"https://api.studio.nebius.ai/v1/\",\n",
        "    api_key=os.environ.get(\"NEBIUS_API_KEY\"),\n",
        ")\n",
        "\n",
        "llama_model = \"meta-llama/Llama-3.3-70B-Instruct\"\n",
        "\n",
        "def prettify_string(text, max_line_length=80):\n",
        "    \"\"\"Prints a string with line breaks at spaces to prevent horizontal scrolling.\n",
        "\n",
        "    Args:\n",
        "        text: The string to print.\n",
        "        max_line_length: The maximum length of each line.\n",
        "    \"\"\"\n",
        "\n",
        "    output_lines = []\n",
        "    lines = text.split(\"\\n\")\n",
        "    for line in lines:\n",
        "        current_line = \"\"\n",
        "        words = line.split()\n",
        "        for word in words:\n",
        "            if len(current_line) + len(word) + 1 <= max_line_length:\n",
        "                current_line += word + \" \"\n",
        "            else:\n",
        "                output_lines.append(current_line.strip())\n",
        "                current_line = word + \" \"\n",
        "        output_lines.append(current_line.strip())  # Append the last line\n",
        "    return \"\\n\".join(output_lines)\n",
        "\n",
        "def answer_with_llm(prompt: str,\n",
        "                    system_prompt=\"You are a helpful assistant\",\n",
        "                    max_tokens=512,\n",
        "                    client=nebius_client,\n",
        "                    model=llama_model,\n",
        "                    prettify=True,\n",
        "                    temperature=None) -> str:\n",
        "\n",
        "    messages = []\n",
        "\n",
        "    if system_prompt:\n",
        "        messages.append(\n",
        "            {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": system_prompt\n",
        "            }\n",
        "        )\n",
        "\n",
        "    messages.append(\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": prompt\n",
        "        }\n",
        "    )\n",
        "\n",
        "    completion = client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=messages,\n",
        "        max_tokens=max_tokens,\n",
        "        temperature=temperature\n",
        "    )\n",
        "\n",
        "    if prettify:\n",
        "        return prettify_string(completion.choices[0].message.content)\n",
        "    else:\n",
        "        return completion.choices[0].message.content\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7gYo9JE6jstU"
      },
      "source": [
        "# Understanding LLM workflows"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QII9FvTsoD6v"
      },
      "source": [
        "## Chaining\n",
        "\n",
        "The most basic LLM workflow type is **chaining**: using several LLM calls in a sequence, the next one modifying or refining the previous ones.\n",
        "\n",
        "<center>\n",
        "<img src=\"\n",
        "https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F7418719e3dab222dccb379b8879e1dc08ad34c78-2401x1000.png&w=3840&q=75\" width=1000 />\n",
        "\n",
        "[Source](https://www.anthropic.com/engineering/building-effective-agents)\n",
        "</center>\n",
        "\n",
        "Example use cases might include:\n",
        "\n",
        "* **Localization**. Though LLMs develop towards multiliguality, it's still easier for them to answer complex instructions in English (because most of the training data, as most of web and books, is in English). A natural way of dealing with that is making a **chain**:\n",
        "\n",
        "  * The first LLM call translates the query from the source language into English\n",
        "  * The second one processes the query in English\n",
        "  * The third one translates the answer back into the source language.\n",
        "\n",
        "Before LLMs became good at structured outputs, another popular use case for chaining was (Answering the question) -> (Extracting the answer).\n",
        "\n",
        "In many cases, however, the workflows arent' sequential, so let's discuss several more comlplex types."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_fQIfRbPoCOg"
      },
      "source": [
        "## Parallelization\n",
        "\n",
        "**Parallelization** is the workflow type where several workers process the query and their outputs are put together by an agregator to produce the final answer.\n",
        "\n",
        "<center>\n",
        "<img src=\"\n",
        "https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F406bb032ca007fd1624f261af717d70e6ca86286-2401x1000.png&w=3840&q=75\" width=1000 />\n",
        "\n",
        "[Source](https://www.anthropic.com/engineering/building-effective-agents)\n",
        "</center>\n",
        "\n",
        "A special case of parallelization is what can be called **LLM MapReduce**. As an example, let's consider long document summarization. if the input is too large to be processed efficiently in one call, we can\n",
        "\n",
        "* distribute input chunks between identical LLM workers (**map** phase)\n",
        "* then ask another LLM to put summaries of individual chunks together (**recude** phase)\n",
        "\n",
        "Another example might be **evaluation of chatbot conversations**. Usually, you want to evaluate your chatbot's proficiency along several axes: helpfulness, tone of voice etc - and all this can be scored by **LLM-as-a-Judge**. And generally it might be a good idea to score different parameters in different and parallel LLM calls - this way the prompts will be simpler and the judges' outputs more reliable. In such a system, Aggregator is optional; you can just put all the scores together without any additional LLMs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u-yy85QCPxcf"
      },
      "source": [
        "## Routing\n",
        "\n",
        "A customer support chatbot may have a complex, tree-like logic switching a user between several conversation branches. You might just prompt one LLM thoroughy and let it rule it out in a chat mode, but if you can describe all the scenarios, why not make things more reliable by creating a **routing** workflow?\n",
        "\n",
        "<center>\n",
        "<img src=\"\n",
        "https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F5c0c0e9fe4def0b584c04d37849941da55e5e71c-2401x1000.png&w=3840&q=75\" width=1000 />\n",
        "\n",
        "[Source](https://www.anthropic.com/engineering/building-effective-agents)\n",
        "</center>\n",
        "\n",
        "In the most basic implementation, the entry-point LLM chooses between several prescribed scenarios based on the user's request. But the workflow might as well be a more complicated one:\n",
        "\n",
        "<center>\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1QoIL5j2C6U5u2gE_jGcF-w3e1qlgWL7d\" width=600 />\n",
        "</center>\n",
        "\n",
        "Not all of the links must be other LLMs. Some might be rule-based processors or even involve human support specialists jumping in to help the client.\n",
        "\n",
        "Another possible application of routing is choosing between a number of LLMs of various capability. For example, you could use a 8B model for simple questions or 70B model for more elaborate ones. Classifying the question's complexity might be a job for an LLM."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "74CyuxOtXRGv"
      },
      "source": [
        "## Feedback loops\n",
        "\n",
        "In complex tasks we might expect that an LLM workflow wouldn't immediately arrive at the final solution. A good example is coding: the first solution may be flawed, and one or two rounds of self-analysis and self-correction might help.\n",
        "\n",
        "If you can describe the evaluation criteria, you can construct a **feedback loop** that would run until the evaluator gives the solution a pass or until the system hits max number of iterations.\n",
        "\n",
        "<center>\n",
        "<img src=\"\n",
        "https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F14f51e6406ccb29e695da48b17017e899a6119c7-2401x1000.png&w=3840&q=75\" width=1000 />\n",
        "\n",
        "[Source](https://www.anthropic.com/engineering/building-effective-agents)\n",
        "</center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gHwMtv8lO6K0"
      },
      "source": [
        "## Creating more complex workflows. Workflows vs agents\n",
        "\n",
        "From these four primitives, you can assemble workflows of arbitrary complexity. For example, here is a potential advertisement creation workflow:\n",
        "\n",
        "<center>\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=14DDGHYOojUClbcI59WRn5lZ_4rAWHpoW\" width=600 />\n",
        "</center>\n",
        "\n",
        "LLM workflows are all human-designed. To create one, you need to come up with the process nodes and connections between them. At times, you would want something - an LLM! - to orchestrate everything for you.\n",
        "\n",
        "Like this:\n",
        "\n",
        "<center>\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=102GUavLMfYjqR0SftsQa5VItH2TEA0JS\" width=400 />\n",
        "</center>\n",
        "\n",
        "LLM-powered orchestration makes the system into an **LLM Agent**. It's a cool and powerful thing, and we'll discuss it in more details in notebooks [A.1](https://colab.research.google.com/github/Nebius-Academy/LLM-Engineering-Essentials/blob/main/topic2/a.1_llm_tools_and_agents.ipynb) and [A.2](https://colab.research.google.com/github/Nebius-Academy/LLM-Engineering-Essentials/blob/main/topic2/a.1_llm_tools_and_agents.ipynb). However, it makes things less transparent and less reliable in comparison with manually orchestrated pipelines.\n",
        "\n",
        "In the rest of this notebook, we'll work out several particular examples of LLM orchestration: summarization and localization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b5OD8m0mrMTU"
      },
      "source": [
        "# LLM workflow examples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GpSCGMNYBjk-"
      },
      "source": [
        "## Summarization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aD14FCgDEBkw"
      },
      "source": [
        "Let's try to write a simple LLM summarization script. As an example text we'll take an article from Wikipedia about paws.\n",
        "\n",
        "Note: we'll take a different model here, specifically **deepseek-ai/DeepSeek-R1** because it's tokenizer doesn't have additional license requirements."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "DnneaYB5MrsO"
      },
      "outputs": [],
      "source": [
        "import requests, bs4\n",
        "\n",
        "content = requests.get(\"https://en.wikipedia.org/wiki/Paw\").content\n",
        "parsed = bs4.BeautifulSoup(content)\n",
        "content_div = parsed.find(\"div\", \"mw-content-container\")\n",
        "full_text = content_div.get_text()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "full_text"
      ],
      "metadata": {
        "id": "ixDNOQOYOd0P",
        "outputId": "f14de530-4a51-4e93-c7f1-1a529016c6f1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n\\n\\n\\n\\n\\n\\nToggle the table of contents\\n\\n\\n\\n\\n\\n\\n\\nPaw\\n\\n\\n\\n33 languages\\n\\n\\n\\n\\nAragonésБългарскиDanskDeutschEestiΕλληνικάفارسیFrançaisGaeilgeJawaಕನ್ನಡKreyòl ayisyenKurdîLingálaLombard日本語Norsk bokmålNorsk nynorskPlattdüütschPolskiPortuguêsРусскийSimple EnglishСловѣньскъ / ⰔⰎⰑⰂⰡⰐⰠⰔⰍⰟکوردیСрпски / srpskiSuomiSvenskaTürkçeУкраїнськаWest-Vlamsייִדיש中文\\n\\nEdit links\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nArticleTalk\\n\\n\\n\\n\\n\\nEnglish\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nReadView sourceView history\\n\\n\\n\\n\\n\\n\\n\\nTools\\n\\n\\n\\n\\n\\nTools\\nmove to sidebar\\nhide\\n\\n\\n\\n\\t\\tActions\\n\\t\\n\\n\\nReadView sourceView history\\n\\n\\n\\n\\n\\n\\t\\tGeneral\\n\\t\\n\\n\\nWhat links hereRelated changesUpload filePermanent linkPage informationCite this pageGet shortened URLDownload QR code\\n\\n\\n\\n\\n\\n\\t\\tPrint/export\\n\\t\\n\\n\\nDownload as PDFPrintable version\\n\\n\\n\\n\\n\\n\\t\\tIn other projects\\n\\t\\n\\n\\nWikimedia CommonsWikidata item\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAppearance\\nmove to sidebar\\nhide\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nFrom Wikipedia, the free encyclopedia\\n\\n\\nSoft foot-like part of a mammal that has claws\\nThis article includes a list of references, related reading, or external links, but its sources remain unclear because it lacks inline citations. Please help improve this article by introducing more precise citations. (November 2022) (Learn how and when to remove this message)\\nFor other uses, see Paw (disambiguation).\\nRight front paw of dog showing A) claw, B) digital pads, C) metacarpal pad, D) dewclaw, E) carpal pad.A paw is the soft foot-like part of a mammal, generally a quadruped, that has claws.\\n\\nCommon characteristics\\nThe paw is characterised by thin, pigmented, keratinised, hairless epidermis covering subcutaneous collagenous and adipose tissue, which make up the pads. These pads act as a cushion for the load-bearing limbs of the animal. The paw consists of the large, heart-shaped metacarpal or palmar pad (forelimb) or metatarsal or plantar pad (rear limb), and generally four load-bearing digital pads, although there can be five or six toes in the case of domestic cats and bears (including giant panda). A carpal pad is also found on the forelimb which is used for additional traction when stopping or descending a slope in digitigrade species. Additional dewclaws can also be present.\\nThe paw also includes a horn-like, beak shaped claw on each digit. Though usually hairless, certain animals do have fur on the soles of their paws.  An example is the red panda, whose furry soles help insulate them in their snowy habitat.\\n\\nAnimals with paws\\nFelids, such as cats and tigers; some of these animals may have toe tufts\\nCanids, such as dogs and foxes\\nRabbits and other lagomorphs have paws with very sharp nails and have no pads underneath them\\nBears and raccoons\\nWeasels and other mustelids\\nRodents\\nGallery\\n\\n\\n\\nAn American brown bear\\'s paws.\\n\\n\\n\\nA polar bear\\'s paws.\\n\\n\\n\\nA dog\\'s paw resting on a hard concrete surface\\n\\n\\n\\nStructures of the leg and paw of a dog.\\n\\n\\n\\nA wolf\\'s paws\\n\\n\\n\\nA tiger\\'s paw, showing pads\\n\\n\\n\\nA cat\\'s paw, showing pads.\\n\\n\\n\\nStructures of the paw of a cat.\\n\\n\\n\\nA rabbit\\'s foot.\\n\\n\\n\\nThe hind paws of a Japanese hare.\\n\\n\\n\\nA red panda\\'s paw, showing the absence of paw pads.\\n\\n\\n\\nThe paw of a yellow-throated marten.\\n\\n\\n\\nThe forepaw of a European badger.\\n\\n\\n\\nFront paw of European hamster\\n\\n\\nSee also\\n\\n\\n\\nWikimedia Commons has media related to Animal feet.\\n\\nClaw\\nDigitigrade\\nReferences\\n\\n\\nBibliography\\nMaine Coon Polydactyl International\\n\\n\\n\\n\\nRetrieved from \"https://en.wikipedia.org/w/index.php?title=Paw&oldid=1211080493\"\\nCategory: Mammal anatomyHidden categories: Articles with short descriptionShort description matches WikidataArticles lacking in-text citations from November 2022All articles lacking in-text citationsCommons category link is on Wikidata\\n\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sxwnjZO5FUx4"
      },
      "source": [
        "We are using **BeautifulSoup** to parse out the contents of the page omitting everything except for the main text.\n",
        "\n",
        "Now let's write a simple llm summarization code:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "OGCE-6cREIcz"
      },
      "outputs": [],
      "source": [
        "model = \"deepseek-ai/DeepSeek-R1\"\n",
        "\n",
        "def summarize_with_llm(text):\n",
        "    chat_completion = nebius_client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=[{\"role\": \"user\", \"content\": f\"Summarize the most important aspects of the following text: {text}. Try to be short.\"}]\n",
        "    )\n",
        "    return chat_completion.choices[0].message.content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "DgZUmZNBH3yG",
        "outputId": "4164a4e6-07c7-461d-df5f-a8a321e26b13",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"<think>\\nOkay, I need to summarize the Wikipedia article about paws. Let me read through the content first. \\n\\nThe article starts by defining a paw as the soft, foot-like part of a mammal with claws, typically in quadrupeds. It mentions common characteristics: the structure includes pads made of keratinized epidermis, collagen, and fat that cushion the limbs. The main parts are the metacarpal (front) or metatarsal (rear) pad, digital pads, a carpal pad for traction, and claws. Some animals like cats and bears might have more toes. Examples given include felids (cats), canids (dogs), rabbits, bears, raccoons, etc. There's a gallery with images of different animals' paws. The article also notes unique features, like red pandas having furry soles for insulation.\\n\\nImportant aspects to highlight: definition of a paw, structure (pads, claws), functions (cushioning, traction), examples of animals with paws, and some unique adaptations. Also, mention the lack of inline citations as a note about the article's reliability. Since the user wants it short, avoid details like the gallery or specific mentions of each animal. Focus on key points: structure, function, examples, and unique traits.\\n</think>\\n\\nThe Wikipedia article on **paws** outlines their key features and functions:  \\n- **Definition**: Soft, clawed foot-like structures in mammals, especially quadrupeds.  \\n- **Structure**: Composed of hairless, keratinized pads (metacarpal/metatarsal, digital, carpal) that cushion limbs, along with claws. Some species (e.g., cats, bears) have extra toes or dewclaws.  \\n- **Function**: Pads absorb shock, provide traction; carpal pads aid in stopping/descending.  \\n- **Variations**: Red pandas have fur-covered soles for insulation; rabbits lack pads but have sharp nails.  \\n- **Examples**: Found in felids (cats), canids (dogs), bears, rodents, and lagomorphs (rabbits).  \\n\\n*Note: The article lacks inline citations, which may affect its reliability.*\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "summarize_with_llm(full_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D6YYJwhKIdF0"
      },
      "source": [
        "This looks like a good summary of the article. However, this is the simplest example. Let's try to do something a bit more complicated."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LgUoE7E1Ijzn"
      },
      "source": [
        "## Map reduce summarization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Lf-fN9YnH6a6"
      },
      "outputs": [],
      "source": [
        "def get_wiki_text(url):\n",
        "    content = requests.get(url).content\n",
        "    parsed = bs4.BeautifulSoup(content)\n",
        "    content_div = parsed.find(\"div\", \"mw-content-container\")\n",
        "    full_text = content_div.get_text()\n",
        "    return full_text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GKgKsLA9JA7H"
      },
      "source": [
        "A page on *2023 in American television* is considered one of the longest pages on Wikipedia. It's mostly long because it's a list of all shows released that year with descriptions. However, it's great for us to test our long text summarization skills."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "kQgIfjREIrXI"
      },
      "outputs": [],
      "source": [
        "full_text = get_wiki_text(\"https://en.wikipedia.org/wiki/2023_in_American_television\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pp-67kasJRHe"
      },
      "source": [
        "The length of the text is not that interesting to us as the number of tokens. Let's try to look at both.\n",
        "\n",
        "We'll use **huggingface** to get the model's tokenizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "8TyFgRnQJlU3"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "\n",
        "def get_token_count(text):\n",
        "    encoding = AutoTokenizer.from_pretrained(model)\n",
        "\n",
        "    encoded = encoding.encode(text)\n",
        "    return len(encoded)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "4euitvkoJz3o",
        "outputId": "082b1cd4-3201-4397-8393-661fd42decfb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237,
          "referenced_widgets": [
            "ca1ab29b864b4afe932bad700a994877",
            "d202a48f7d224a28afae3a46f36a3b4e",
            "e9ff11204ad348b28d38c6a2815d62e9",
            "eaa834edfef447bebb59175087d0fabc",
            "87151a8dd49342b5b67caa8b61da6e93",
            "c2ec9a653591428c8909b6ecfab7a761",
            "244276ebc3c5459aa5ca9bf5a122e300",
            "a1884290a5b044b8a565100d41f7cffe",
            "b0b76f75336e4a0ab42758d352715dd8",
            "14280a575e374bab9845a27c3149a5b2",
            "078e22c4c9854283a11c0460cf8cd6ff",
            "3bf80215ce84430fb07ce1ab456ff38e",
            "8633ca39588a4b2ca4e81447060721d6",
            "c0098378e8f5450bbb69e7d65ef274e8",
            "4768ce9c885446aeba6c7771567615b1",
            "898f4d534bf64ab5aa987511c77e3a85",
            "12254dc89ddc47f1b9967b3a0322ad01",
            "cdbe437fe4c54f7eb6b15d0233541ff9",
            "d18657d9f2b849629d9a0da08c67d8b9",
            "6c7bcb0b30864f80a578863649fff9c3",
            "8cbd221849064af586c409f99b4f9753",
            "675c53ed0e33494890879438f06e00ff"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/3.59k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ca1ab29b864b4afe932bad700a994877"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/7.85M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3bf80215ce84430fb07ce1ab456ff38e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (105179 > 16384). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of tokens: 105179\n",
            "Number of characters: 399168\n"
          ]
        }
      ],
      "source": [
        "print(f\"Number of tokens: {get_token_count(full_text)}\")\n",
        "print(f\"Number of characters: {len(full_text)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WiSEf3CKKEYO"
      },
      "source": [
        "Even though technically DeepSeek R1 can take this whole text at once (it has 164000 token-long context window), it's usually not ideal. Especially because for models computations grow more than linearly proportionally to length. So it's better to summarize in **MapReduce** style, where we summarize small parts and then generate a summary for the whole text. It can also help us summarize texts which are larger than our context window.\n",
        "\n",
        "Let's experiment with both:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "zOQDGlMDJ2Fg",
        "outputId": "45c78d48-3609-4e34-cd58-2a3a40918996",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<think>\\nOkay, I need to summarize the main points of this lengthy Wikipedia article about 2023 in American television. First, I\\'ll skim through the article to identify the most important events and sections. The article is structured with a list of notable events each month, followed by sections on TV shows, network changes, milestones, deaths, and other related topics.\\n\\nStarting with the Notable Events section, each month has several key happenings. For instance, in January, there was the Damar Hamlin incident during a NFL game, the rebranding of KCBS-TV and KCAL-TV, and the Writers Guild of America strike announcement. February included the Super Bowl LVII and Rihanna\\'s halftime show. March had the Kids\\' Choice Awards and the bankruptcy filing of Diamond Sports Group. April saw the end of Tucker Carlson\\'s show on Fox News and the Writers Guild strike beginning. May had the WGA strike impacting production, and June included the NBA Finals and changes at CNN. Later months covered strikes, channel rebrands, and significant show cancellations or endings.\\n\\nThe TV shows section lists debuts, network changes, milestones, and endings. It\\'s a lot, but the key points would be major show finales like \"Succession,\" \"The Marvelous Mrs. Maisel,\" and \"Titans.\" Also, notable show returns and cancellations. The networks and services section outlines new launches like CBS News Detroit and closures like HBO Max rebranding to Max. The deaths section lists numerous TV personalities who passed away in 2023, which might be important to mention briefly.\\n\\nConsidering the user wants a concise summary, I should focus on the most impactful events: major strikes (WGA and SAG-AFTRA), significant show endings and premieres, major sports events (Super Bowl, NBA Finals), network changes and mergers, notable deaths, and the shift towards streaming services. Also, mention key controversies or disputes like carriage issues between providers and networks.\\n\\nI need to avoid getting bogged down in every single event but highlight the ones with the broadest impact. The writers\\' and actors\\' strikes would be crucial as they affected production across the industry. The NFL\\'s record viewership and the Super Bowl details are significant. The rebranding of HBO Max to Max and other network changes like Nexstar acquiring The CW are important business moves. Notable deaths like Jerry Springer and others should be included.\\n\\nI\\'ll structure the summary by first mentioning the key events and strikes, then notable shows and network changes, followed by sports highlights, and finally a brief mention of notable deaths. Keeping it under the required token limit, concise but informative.\\n</think>\\n\\n**2023 in American Television: Key Highlights**  \\n\\n**Major Industry Events:**  \\n- **Writers Guild (WGA) & SAG-AFTRA Strikes**: A dual strike from May to November disrupted production, delaying shows and impacting fall TV schedules. It was the first simultaneous strike since 1960, driven by disputes over streaming residuals and AI use.  \\n- **Sports Milestones**:  \\n  - Super Bowl LVII (Chiefs vs. Eagles) broke records with 123.7 million viewers, including Rihanna’s halftime show.  \\n  - NBA Finals saw the Denver Nuggets win their first title, while the Vegas Golden Knights claimed their first Stanley Cup.  \\n\\n**Network Shifts & Streaming:**  \\n- **Rebranding**: HBO Max became *Max* (merging with Discovery+), while Epix rebranded as MGM+.  \\n- **Nexstar’s Expansion**: Acquired The CW, leading to affiliate changes (e.g., WGN-TV, WPIX) and launching ACC football/basketball coverage.  \\n- **Streaming Growth**: NFL Sunday Ticket moved to YouTube TV, and Netflix cracked down on password sharing.  \\n\\n**Notable Show Changes:**  \\n- **Endings**: *Succession*, *The Marvelous Mrs. Maisel*, *Titans*, *Tucker Carlson Tonight*, and *The Flash* concluded.  \\n- **Revivals/Reboots**: *Night Court*, *Frasier*, and *Futurama* returned, while *True Lies* and *The Wonder Years* were canceled.  \\n- **Controversies**: Jason Aldean’s “Try That in a Small Town” video pulled by CMT for racial undertones.  \\n\\n**Business & Legal:**  \\n- **Diamond Sports Group Bankruptcy**: Affected regional sports networks (Bally Sports), prompting MLB to take over Padres and Diamondbacks broadcasts.  \\n- **Carriage Disputes**: DirecTV dropped Nexstar and Tegna stations temporarily; Sinclair cut local news at multiple affiliates.  \\n\\n**Deaths**: Notable losses included Jerry Springer (host), Lance Reddick (*The Wire*), Treat Williams (*Everwood*), Cindy Williams (*Laverne & Shirley*), and Suzanne Somers (*Step by Step*).  \\n\\n**Awards & Ratings**:  \\n- Emmy Awards delayed to January 2024 due to strikes.  \\n- *The Bear* and *Succession* dominated awards; *Everything Everywhere All At Once* swept film categories.  \\n\\n2023 marked a transformative year with labor actions reshaping industry norms, streaming consolidating power, and sports continuing to dominate viewership.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "naive_summary = summarize_with_llm(full_text)\n",
        "naive_summary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zRpBT5E9LA2K"
      },
      "source": [
        "To orchestrate a MapReduce pipeline, we'll need to break text down into chunks - and we don't want it to be sliced mid-sentence. So, we'll use special tools for that.\n",
        "\n",
        "Langchain has a handy tool called `TextSplitter` which allows to split a text following specific rules.\n",
        "\n",
        "For example, `RecursiveCharacterTextSplitter` can split texts recursively based on list of characters until it reaches desired length. It also allows you to set up overlap so that chunks have some connections between each other. It's a useful thing, but we will not be using this here.\n",
        "\n",
        "The default delimiter list is `[\"\\n\\n\", \"\\n\", \" \", \"\"]`. Which in theory gives us splitting by paragraphs, subparagraphs, words and then characters.\n",
        "\n",
        "Langchain also allows you to define length functions for Splitters, which will be used to determine if the chunk is of an appropriate length. We can even instantiate a length function from `tiktoken` encoder directly, so that our chunk length is tied to the token count."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "iZvdvqoCOXJd",
        "outputId": "fe6b0cc5-3e2c-45a4-cf81-622c6be83f22",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/65.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m61.4/65.2 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.2/65.2 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/438.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m \u001b[32m430.1/438.1 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m438.1/438.1 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/363.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.0/363.0 kB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -qU langchain langchain-openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "hNfh3NYpKTfn"
      },
      "outputs": [],
      "source": [
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "splitter = RecursiveCharacterTextSplitter.from_huggingface_tokenizer(\n",
        "    tokenizer=AutoTokenizer.from_pretrained(model),\n",
        "    chunk_size=10000,\n",
        "    chunk_overlap=0\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "4GbvfqFoLRN9",
        "outputId": "940a2dd7-d7e7-405c-ac75-098406eb178f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "11"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "splitted_text = splitter.split_text(full_text)\n",
        "len(splitted_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FQRYowhRNDqY"
      },
      "source": [
        "Let's create our Map and Reduce operations.\n",
        "\n",
        "Notice that langchain uses a bit of a different notation for it's chains."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "PPZBpVc4Mltc"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "llm = ChatOpenAI(\n",
        "    model=llama_model,\n",
        "    base_url=\"https://api.studio.nebius.ai/v1/\",\n",
        "    api_key=os.environ['NEBIUS_API_KEY']\n",
        ")\n",
        "\n",
        "map_prompt = ChatPromptTemplate.from_messages(\n",
        "    [(\"human\", \"Write a concise summary of the following:\\\\n\\\\n{context}\")]\n",
        ")\n",
        "\n",
        "map_chain = map_prompt | llm | StrOutputParser()\n",
        "\n",
        "\n",
        "reduce_template = \"\"\"\n",
        "The following is a set of summaries:\n",
        "{docs}\n",
        "Take these and distill it into a final, consolidated summary\n",
        "of the main themes.\n",
        "\"\"\"\n",
        "\n",
        "reduce_prompt = ChatPromptTemplate([(\"human\", reduce_template)])\n",
        "\n",
        "reduce_chain = reduce_prompt | llm | StrOutputParser()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "map_chain"
      ],
      "metadata": {
        "id": "hlYQB96lPeV_",
        "outputId": "af9c5504-7bca-4e73-9ae4-00e69df42d6e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatPromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template='Write a concise summary of the following:\\\\n\\\\n{context}'), additional_kwargs={})])\n",
              "| ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7b7769c1e790>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7b7769c677d0>, root_client=<openai.OpenAI object at 0x7b7769c42990>, root_async_client=<openai.AsyncOpenAI object at 0x7b7769c67450>, model_name='meta-llama/Llama-3.3-70B-Instruct', model_kwargs={}, openai_api_key=SecretStr('**********'), openai_api_base='https://api.studio.nebius.ai/v1/')\n",
              "| StrOutputParser()"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g9GqG47tOE2t"
      },
      "source": [
        "Now, in the simplest form our MapReduce summarization would look like this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "YjcYjF4jPGOH"
      },
      "outputs": [],
      "source": [
        "from tqdm.auto import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "Mv4L1-6oOHpC"
      },
      "outputs": [],
      "source": [
        "def map_reduce_summarization(docs):\n",
        "    summaries = map(\n",
        "        lambda doc: map_chain.invoke({\"context\": doc}),\n",
        "        tqdm(docs)\n",
        "    )\n",
        "\n",
        "    final_summary = reduce_chain.invoke({\"docs\": \"\\n\\n\".join(summaries)})\n",
        "    return final_summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "ydwlpynMOQk0",
        "outputId": "f7a2584c-3404-4728-a7bf-ca3633c8b057",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 137,
          "referenced_widgets": [
            "73a072d7b63b415098e10f1853664dab",
            "2d1586852c7c44bdb20671a40246637e",
            "8f0a6f514d7f4a43bac185753de7ba60",
            "106060b961d74ac48e555082c6cd6347",
            "d920d07dbaf744d29e5e1de8217b5881",
            "e210916cf612435c81e2aa2deca1e280",
            "117ffc0559cc4c65ae08a7bcec021e5c",
            "3717e6d8928c4039a1110a8c10f02734",
            "9cc39c3b77a1449da756071b69f2afce",
            "9338e76d51be4aeabaae20a70bf5e130",
            "d07207007c45438c826805cf1cdcfa00"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/11 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "73a072d7b63b415098e10f1853664dab"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'After analyzing the provided text, I have identified several main themes that are present throughout the various summaries. Here is a consolidated summary of the main themes:\\n\\n**Television Industry Changes**\\n\\n* Numerous TV shows were renewed, canceled, or ended in 2023, including notable shows such as \"The Blacklist,\" \"The Crown,\" and \"Archer.\"\\n* TV networks and streaming services underwent changes, including the rebranding of HBO Max as \"Max\" and the launch of new FAST channels by NBCUniversal.\\n* Several TV stations changed affiliations, and new networks were launched, such as \"The Nest\" by Sinclair.\\n\\n**Notable Deaths in the Entertainment Industry**\\n\\n* Unfortunately, 2023 saw the passing of many notable individuals in the entertainment industry, including actors, musicians, producers, and athletes. Some notable deaths include Paul Reubens (Pee-wee Herman), Tony Bennett, Matthew Perry, and Bob Barker.\\n\\n**Awards Shows and Events**\\n\\n* Several awards shows took place in 2023, including the Golden Globes, the Grammy Awards, the Academy Awards, and the Emmy Awards.\\n* The 2023 Kids\\' Choice Awards, the MTV Movie & TV Awards, and the ESPYS also took place, honoring top performers in their respective fields.\\n\\n**Streaming Services and Online Content**\\n\\n* Streaming services continued to grow and evolve in 2023, with new launches, rebranding, and changes in content offerings.\\n* Online content platforms, such as YouTube and social media, played a significant role in shaping the entertainment industry in 2023.\\n\\n**Sports Media and Broadcasting**\\n\\n* The sports media landscape underwent changes in 2023, including new broadcast deals, changes in sports network ownership, and the launch of new sports streaming services.\\n* The NFL, NBA, and MLB all had significant developments in their broadcast and streaming rights in 2023.\\n\\n**Labor Disputes and Industry Trends**\\n\\n* The entertainment industry experienced labor disputes in 2023, including the Writers Guild of America (WGA) strike, which affected TV and film production.\\n* The industry also saw trends towards increased diversity and representation, as well as a growing focus on streaming and online content.\\n\\nOverall, 2023 was a significant year for the entertainment industry, with many changes, developments, and notable events taking place.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "map_reduce_summarization(splitted_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JEAzpF6TfmjA"
      },
      "source": [
        "## Map reduce with LLM orchestration\n",
        "\n",
        "In some cases we might need a bit more complicated orchestration of MapReduce calls. Let's imagine an example scenario.\n",
        "\n",
        "We want to create a party of adventurers. We can decompose this task into multiple steps. Namely:\n",
        "\n",
        "1. Generate a rought outline of the party, amount of members, rough descriptions.\n",
        "2. For each member generate full story and skill list\n",
        "3. Gather all descriptions and generate a short story of how they came to be together.\n",
        "\n",
        "<center>\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1ScixZwB5AK9TQWrpIg5nTErT_M0Y-S9C\" width=600 />\n",
        "</center>\n",
        "\n",
        "For part 2 we can reuse our previous example from the structured generation notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "X9N2kpsjOSUp"
      },
      "outputs": [],
      "source": [
        "from typing import List\n",
        "from pydantic import BaseModel\n",
        "\n",
        "class CharacterProfile(BaseModel):\n",
        "    name: str\n",
        "    age: int\n",
        "    special_skills: List[str]\n",
        "    traits: List[str]\n",
        "    character_class: str\n",
        "    origin: str\n",
        "\n",
        "def generate_character(description: str):\n",
        "    completion = nebius_client.beta.chat.completions.parse(\n",
        "        model=llama_model,\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": \"Design a role play character based on the following\"\\\n",
        "                          f\"short description {description}\"}\n",
        "        ],\n",
        "        extra_body={\n",
        "            \"guided_json\": CharacterProfile.model_json_schema()\n",
        "        }\n",
        "    )\n",
        "\n",
        "    return CharacterProfile.model_validate_json(completion.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "p1RG3QcwgWlQ",
        "outputId": "c4fdf1aa-34a4-4a6d-fd05-838e0d619870",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CharacterProfile(name='Gorthok the Unyielding', age=32, special_skills=['Intimidation', 'Frenzied Attack', 'Survival'], traits=['Fearless', 'Resilient', 'Fierce'], character_class='Berserker', origin='Norse Warrior')"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "generate_character(\"Human berserker\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nY0_iuBRg8-P"
      },
      "source": [
        "Now, for steps 1 and 3:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "tJ1Tr5EHgypK"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "def pregenerate_party():\n",
        "    json_output = nebius_client.chat.completions.create(\n",
        "        messages=[{\n",
        "            'role': 'user', \\\n",
        "            'content': 'Generate a short description for a party of adventurers.\\n'\\\n",
        "            'A party should have 3-5 adventures and a balanced classes set, i.e. '\\\n",
        "            'have at least a melee tank, a support and a damage dealer. \\n'\\\n",
        "            'One of the characters must be thcick hot horny woman. \\n'\\\n",
        "            'Output those short descriptions in a json format as a list with the key \"party\".\\n'\\\n",
        "            'Each description should be a string with only a couple of details'\n",
        "\n",
        "        }],\n",
        "        model=llama_model,\n",
        "        response_format={\"type\": \"json_object\"}\n",
        "    ).choices[0].message.content\n",
        "    return json.loads(json_output)['party']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "Xqd2c_ALhvyY",
        "outputId": "6d6a115e-e5e7-4c4b-d25f-e415541060ca",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Eilif Stonefist, a sturdy dwarf cleric, leads a party with Arin the Bold, an elven rogue, and Lyra Earthsong, a voluptuous half-elf druid, known for her untamed passion and fierce combat skills.',\n",
              " 'The alluring warrior-priestess, Galenna Shadowglow, a curvaceous human female, fights alongside her companions, the cunning halfling bard, Finnley Swiftfoot, and the dwarf paladin, Morgran Ironfist.',\n",
              " 'In the land of Eldoria, a group of brave adventurers emerges, consisting of the seductive sorceress, Xanthea Firehair, a ravishingly beautiful woman with unparalleled magical prowess, the human fighter, Thrain Blackwood, and the elf monk, Althaeon Starseeker.',\n",
              " 'With their combined strength, the party of Keira Emberfist, a stunningly attractive dwarf barbarian, the charming half-elf bard, Elwynn Moonwhisper, and the mysterious gnome wizard, Zorvath, are ready to face any challenge that comes their way.']"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ],
      "source": [
        "pregenerate_party()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "oPBKL7g6hxyz"
      },
      "outputs": [],
      "source": [
        "def generate_back_story(party_details: str):\n",
        "    return nebius_client.chat.completions.create(\n",
        "        messages=[\n",
        "            {\n",
        "                'role': 'user', \\\n",
        "                'content': 'Based on the following party details generate '\\\n",
        "                'a short story of how this party came to be together.\\n'\n",
        "                f'{str(party_details)}'\n",
        "            }\n",
        "        ],\n",
        "        model=llama_model,\n",
        "    ).choices[0].message.content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FK9kw1SIitZi"
      },
      "source": [
        "Now to put it all together in a workflow:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "hL2Lzr08isa3"
      },
      "outputs": [],
      "source": [
        "def generate_party():\n",
        "    party = pregenerate_party()\n",
        "\n",
        "    character_sheets = [\n",
        "        generate_character(character)\n",
        "        for character in party\n",
        "    ]\n",
        "\n",
        "    backstory = generate_back_story(character_sheets)\n",
        "\n",
        "    character_sheets_str = \"\\n\".join([\n",
        "        str(character) for character in character_sheets\n",
        "    ])\n",
        "\n",
        "    return f\"\"\"\n",
        "Party description:\n",
        "{json.dumps(party, indent=4)}\n",
        "\n",
        "Party backstory:\n",
        "{backstory}\n",
        "\n",
        "Party members:\n",
        "{character_sheets_str}\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "3Ol8shxnjaeD",
        "outputId": "312719d9-8c12-48bb-faf0-24f55001f4da",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Party description:\n",
            "[\n",
            "    \"Eilif Stonefist, a sturdy dwarf tank, journeys with Elara Moonwhisper, a sultry and cunning half-elf rogue, Lila Earthsong, a voluptuous and charismatic human cleric, and Arin the Bold, a skilled human wizard.\",\n",
            "    \"Kael Darkhaven, a brooding human paladin, fights alongside Lyra Frostbite, a stunning and curvaceous dwarf barbarian, Mira Shadowglow, an enigmatic and agile elf ranger, and Zara Ember, a ravishing and seductive human sorceress.\",\n",
            "    \"Thrain Blackbeard, a grizzled dwarf fighter, leads a party consisting of himself, Freya Battleborn, a fierce and attractive shieldmaiden, Eira Shadowfire, a mysterious and alluring elf bard, and Jax Blackwood, a cunning and deadly human rogue.\",\n",
            "    \"Valoric Stormfist, a mighty human warrior, adventures with Bran Ironfist, a sturdy dwarf cleric, Lirien Greenleaf, a lithe and sensual elf druid, and Niamh Starseeker, a beautiful and sultry half-elf wizard.\",\n",
            "    \"Gorthok the Unyielding, a hulking orc tank, travels with Vorga Bloodfury, a fiercely attractive and deadly orc barbarian, Keir Darkhunter, a rugged and mysterious human hunter, and Xanthea Moonflower, a ravishing and enchanting human bard.\"\n",
            "]\n",
            "\n",
            "Party backstory:\n",
            "It was a dark and stormy night in the bustling tavern of Silverhaven, a human kingdom known for its brave warriors and skilled craftsmen. Eilif Stonefist, a sturdy dwarf from the underground city of Kragnir, sat at the bar, nursing a mug of ale. His Master Craftsman skills had brought him to the surface world, seeking new materials and techniques to bring back to his people. As he sipped his drink, he noticed a group of humans gathered near the fire, discussing a recent surge in dark cult activity in the nearby lands.\n",
            "\n",
            "Among them was Kael Darkhaven, a brooding paladin from the Human Kingdom of Silverleaf. Haunted by his past, Kael had dedicated his life to protecting the innocent and vanquishing evil. His Divine Magic and Martial Combat skills made him a formidable opponent, and his Charismatic Leader trait had drawn a small crowd of admirers.\n",
            "\n",
            "As Eilif watched, a grizzled old dwarf, Thrain Blackbeard, strode into the tavern, his Master Swordsmanship and Tactical Leadership evident in the way he carried himself. A veteran of countless battles, Thrain had come to Silverhaven seeking new challenges and opportunities to prove himself. His Fearless and Loyal traits had earned him a reputation as a trustworthy ally, and he was soon drawn into the conversation with Kael and the others.\n",
            "\n",
            "Just then, a boisterous warrior, Valoric Stormfist, burst into the tavern, his Master Swordsmanship and Shield Mastery on full display as he regaled the crowd with tales of his adventures. A charismatic leader from the Human Kingdom of Silverhaven, Valoric was always looking for the next great challenge, and his Unyielding trait had earned him a reputation as a tenacious fighter.\n",
            "\n",
            "As the night wore on, a hulking figure, Gorthok the Unyielding, emerged from the shadows, his Intimidation and Shield Mastery skills making him a formidable presence. An Orc Tank from the Red Blade Clan, Gorthok had grown tired of the endless battles and bloodshed of his own people, and had set out to find a new purpose in the world. His Loyal and Unyielding traits had led him to seek out like-minded warriors, and he was drawn to the group's discussion of dark cults and the need for brave heroes to combat them.\n",
            "\n",
            "As the storm raged on outside, the five warriors found themselves drawn together by their shared sense of purpose and their desire to protect the innocent. Eilif's Dwarven Resilience and Thrain's Battle-Hardened trait made them a sturdy foundation, while Kael's Divine Magic and Valoric's War Cry provided a powerful offensive punch. Gorthok's Intimidation and Shield Mastery skills rounded out the group, making them a formidable team.\n",
            "\n",
            "And so, the party was formed, united by their quest to vanquish evil and bring hope to a world filled with darkness. With their combined skills and traits, they set out into the stormy night, ready to face whatever challenges lay ahead.\n",
            "\n",
            "Party members:\n",
            "name='Eilif Stonefist' age=250 special_skills=['Master Craftsman', 'Expert Fighter', 'Dwarven Resilience'] traits=['Sturdy', 'Resilient', 'Loyal'] character_class='Tank' origin='Kragnir, the underground dwarven city'\n",
            "name='Kael Darkhaven' age=32 special_skills=['Martial Combat', 'Divine Magic', 'Intimidation'] traits=['Brooding', 'Charismatic Leader', 'Haunted by Past'] character_class='Paladin' origin='Human Kingdom of Silverleaf'\n",
            "name='Thrain Blackbeard' age=250 special_skills=['Master Swordsmanship', 'Tactical Leadership', 'Dwarven Engineering'] traits=['Fearless', 'Loyal', 'Battle-Hardened'] character_class='Fighter' origin='Kragnir, the Dwarven Clans'\n",
            "name='Valoric Stormfist' age=35 special_skills=['Master Swordsmanship', 'Shield Mastery', 'War Cry'] traits=['Fearless', 'Charismatic Leader', 'Unyielding'] character_class='Warrior' origin='Human Kingdom of Silverhaven'\n",
            "name='Gorthok the Unyielding' age=35 special_skills=['Intimidation', 'Shield Mastery', 'Combat Tactics'] traits=['Fearless', 'Unyielding', 'Loyal'] character_class='Orc Tank' origin='Red Blade Clan'\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(generate_party())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1bdXY1qNaI9b"
      },
      "source": [
        "## LLM localization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BoCJvoI9ZcA9"
      },
      "source": [
        "Even if your base LLM is better at English that at your target language, you can easily translate your outputs with another LLM call."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "pncgXMZokqII"
      },
      "outputs": [],
      "source": [
        "def translate_to_language(input: str, target_language: str):\n",
        "    return nebius_client.chat.completions.create(\n",
        "        messages=[\n",
        "            {\n",
        "                'role': 'user', \\\n",
        "                'content': f'Translate the following text into {target_language}:\\n{input}'\n",
        "            }\n",
        "        ],\n",
        "        model=llama_model,\n",
        "    ).choices[0].message.content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "gS5ImGWAZw0r",
        "outputId": "d97bcd08-61fb-48a4-a273-a705f035ce63",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Party description:\n",
            "[\n",
            "    \"Eilif Stonefist, a dwarf cleric, leads a party with Arin the Bold, an elf rogue, Lila Earthsong, a thick hot horny half-orc barbarian, and Elwynn Moonwhisper, an elf wizard.\",\n",
            "    \"The Brave Adventurers: Gorin the Unyielding, a human fighter, protects the group with his melee prowess, while Jax Blackwood, a charming bard, supports the party and seduces enemies, alongside Mira Shadowglow, a sneaky rogue, and Thalia Starseeker, a curvaceous and alluring female dwarf sorceress.\",\n",
            "    \"The Daring Explorers: Althaea Battleborn, a thick and beautiful female warrior, takes point as the melee tank, supported by Eira Shadowleaf, an agile and deadly half-elf ranger, Elara Moonflower, a gentle and sensual healer, and Arden Stargazer, a wise and powerful wizard.\"\n",
            "]\n",
            "\n",
            "Party backstory:\n",
            "It was a chilly winter evening in the bustling town of Kragnir, nestled within the Dwarven Clanhold. The snowflakes gently fell onto the cobblestone streets, casting a serene silence over the crowded taverns and market stalls. Eilif Stonefist, a stalwart cleric, had just finished a long day of tending to the wounded and the weary at the local temple. As he walked through the town, his sturdy frame and resolute demeanor commanded respect from the passing dwarves.\n",
            "\n",
            "As he entered the Silver Stag Inn, the warm glow of the fire pit and the murmur of hushed conversations enveloped him. Eilif's eyes scanned the room, and his gaze landed on a striking young woman with curves that could stop a clock. Thalia Starseeker, the sorceress, sat at the bar, her alluring smile and confident laughter captivating the attention of the patrons. Her elemental magic, though subtle, seemed to dance in the air around her, as if the very stars themselves had descended to mingle with the mortal crowd.\n",
            "\n",
            "Just as Eilif was about to approach Thalia, a commotion erupted near the entrance. A group of rowdy thugs, clearly looking for trouble, stumbled into the inn. The atmosphere darkened, and the patrons exchanged nervous glances. That was when Althaea Battleborn, the fearless warrior, strode into the fray. Her charismatic leadership and unwavering loyalty were evident as she swiftly defused the situation, disarming the thugs and sending them packing.\n",
            "\n",
            "As the dust settled, Eilif, Thalia, and Althaea found themselves at the center of attention. The cleric, impressed by the warrior's bravery, introduced himself and struck up a conversation. Thalia, intrigued by the unlikely duo, slid onto the stool beside them, her curvaceous figure and seductive charm weaving a spell of fascination around the pair.\n",
            "\n",
            "As they talked, their differences became apparent, yet their shared sense of purpose and adventure began to forge an unbreakable bond. Eilif spoke of his divine calling, Thalia of her thirst for magical knowledge, and Althaea of her clan's honor and legacy. The night wore on, and their conversations flowed like the ale, as they discovered that their unique skills and traits complemented each other perfectly.\n",
            "\n",
            "A spark of destiny ignited within them, and they realized that their chance meeting was, in fact, a call to adventure. The cleric's healing prowess, the sorceress's elemental magic, and the warrior's battle strategy would prove to be a formidable combination. As the night drew to a close, they clasped hands, and their party was born. United by fate, they set out to face the challenges that lay beyond the walls of Kragnir, their bond growing stronger with each passing day, as they explored the vast expanse of the unknown together.\n",
            "\n",
            "Party members:\n",
            "name='Eilif Stonefist' age=250 special_skills=['Healing', 'Shield Mastery', 'Divine Intervention'] traits=['Sturdy', 'Resolute', 'Devout'] character_class='Cleric' origin='Kragnir, the Dwarven Clanhold'\n",
            "name='Thalia Starseeker' age=28 special_skills=['Elemental Magic', 'Seduction', 'Persuasion'] traits=['Curvaceous', 'Alluring', 'Confident'] character_class='Sorceress' origin='Dwarven Clan'\n",
            "name='Althaea Battleborn' age=32 special_skills=['Expert Swordsmanship', 'Shield Mastery', 'Battle Strategy'] traits=['Fearless', 'Charismatic Leader', 'Unwavering Loyalty'] character_class='Warrior' origin='Battleborn Clan'\n",
            "\n",
            "Here is the translation of the provided text into Russian:\n",
            "\n",
            "Описание команды:\n",
            "[\n",
            "    \"Эйлиф Каменная Рука, клерик-гном, возглавляет команду, в которую входят Арин Смелый, эльф-вор, Лила Земляная Песня, горячая и страстная половинка-орк-варвар, и Элвинн Лунный Шепот, эльф-маг.\",\n",
            "    \"Храбрые авантюристы: Горин Непокорный, человеческий воин, защищает группу своей ближней мощью, а Джакс Блэквуд, обаятельный бард, поддерживает команду и соблазняет врагов, вместе с Мирой Теневой Свет, хитрой воровкой, и Талией Звездочет, соблазнительной и притягивающей женской гномской колдуньей.\",\n",
            "    \"Смелые исследователи: Алтея Родная Битва, красивая и плотная женская воительница, принимает на себя роль ближнего боя, поддерживаемая Эйрой Теневой Лист, ловкой и смертельной половинкой-эльфийской рейнджер, Эларой Лунным Цветком, нежной и чувственной целительницей, и Арденом Звездочетом, мудрым и могущественным магом.\"\n",
            "]\n",
            "\n",
            "Предыстория команды:\n",
            "Было холодное зимнее вечер в оживленном городе Крагнире, расположенном внутри Горного Дворца Гномов. Снежинки мягко падали на булыжные улицы, создавая тихую тишину над переполненными тавернами и рыночными лавками. Эйлиф Каменная Рука, стойкий клерик,刚 закончил долгий день ухода за ранеными и уставшими в местном храме. Когда он шел через город, его крепкое телосложение и решительный вид внушали уважение проходящим гномам.\n",
            "\n",
            "Когда он вошел в таверну Серебряного Оленя, теплый свет костра и шепот разговоров окутali его. Глаза Эйлифа искали комнату, и его взгляд упал на красивую молодую женщину с фигурой, которая могла остановить часы. Талия Звездочет, колдунья, села за бар, ее соблазнительная улыбка и уверенная смех завораживали внимание посетителей. Ее элементальная магия, хотя и тонкая, казалась танцевать в воздухе вокруг нее, как будто сами звезды спустились, чтобы пообщаться с смертной толпой.\n",
            "\n",
            "Как раз когда Эйлиф собирался подойти к Талии, разгорелась суматоха у входа. Группа шумных хулиганов, rõчно ищущих проблемы, ворвалась в таверну. Атмосфера потемнела, и посетители обменялись нервными взглядами. Именно тогда Алтея Родная Битва, бесстрашная воительница, вышла на сцену. Ее харизматическое лидерство и непоколебимая верность были очевидны, когда она быстро разобралась с ситуацией, разоружив хулиганов и прогнав их.\n",
            "\n",
            "Когда пыль улеглась, Эйлиф, Талия и Алтея оказались в центре внимания. Клерик, впечатленный смелостью воительницы, представился и начал разговор. Талия, заинтересованная неожиданным дуэтом, села на табуретку рядом с ними, ее соблазнительная фигура и обаяние плели заклятие очарования вокруг пары.\n",
            "\n",
            "Когда они разговаривали, их различия стали очевидными, но их общее чувство цели и приключения начало создавать нерушимую связь. Эйлиф говорил о своем божественном призвании, Талия о своем жажде магических знаний, и Алтея о чести и наследии своего клана. Ночная атмосфера разворачивалась, как эль, когда они обнаружили, что их уникальные навыки и черты идеально дополняют друг друга.\n",
            "\n",
            "Искра судьбы запустилась внутри них, и они осознали, что их случайная встреча была, в действительности, призывом к приключению. П般ь клерика, элементальная магия колдуньи и стратегия боя воительницы окажутся могущественным сочетанием. Когда ночь подходила к концу, они сцепили руки, и их команда родилась. Объединенные судьбой, они отправились встретить проблемы, лежащие за пределами стен Крагнира, их связь становилась сильнее с каждым днем, когда они исследовали огромную неизведанность вместе.\n",
            "\n",
            "Участники команды:\n",
            "имя='Эйлиф Каменная Рука' возраст=250 особые навыки=['Исцеление', 'Владение щитом', 'Божественное вмешательство'] черты=['Крепкое телосложение', 'Решительный', 'Преданный'] класс='Клерик' происхождение='Крагнир, Горный Дворец Гномов'\n",
            "имя='Талия Звездочет' возраст=28 особые навыки=['Элементальная магия', 'Соблазнение', 'Убеждение'] черты=['Соблазнительная', 'Притягивающая', 'Уверенная'] класс='Колдунья' происхождение='Горный Дворец Гномов'\n",
            "имя='Алтея Родная Битва' возраст=32 особые навыки=['Экспертное владение мечом', 'Владение щитом', 'Тактика боя'] черты=['Бесстрашная', 'Харизматичный лидер', 'Непоколебимая верность'] класс='Воительница' происхождение='Клан Родной Битвы'\n"
          ]
        }
      ],
      "source": [
        "party = generate_party()\n",
        "print(party)\n",
        "translated_party = translate_to_language(party, \"Russian\")\n",
        "print(translated_party)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "spWdl38glcty"
      },
      "source": [
        "Let's also try asking the LLM to generate the party in Spanish, this omitting the translation stage:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "MOsSyA4gZ4zq"
      },
      "outputs": [],
      "source": [
        "def pregenerate_party_in_language(target_language: str):\n",
        "    json_output = nebius_client.chat.completions.create(\n",
        "        messages=[{\n",
        "            'role': 'user', \\\n",
        "            'content': 'Generate a short description for a party of adventurers.\\n'\\\n",
        "            'A party should have 3-5 adventures and a balanced classes set, i.e. '\\\n",
        "            'have at least a melee tank, a support and a damage dealer. \\n'\\\n",
        "            'One of the characters must be thcick hot horny woman. \\n'\\\n",
        "            'Output those short descriptions in a json format as a list with the key \"party\".\\n'\\\n",
        "            'Each description should be a string with only a couple of details.\\n'\\\n",
        "            f'Generate in {target_language}'\n",
        "        }],\n",
        "        model=llama_model,\n",
        "        response_format={\"type\": \"json_object\"}\n",
        "    ).choices[0].message.content\n",
        "    return json.loads(json_output)['party']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "HVcTUx1laDv9",
        "outputId": "0eed56cd-32bf-4555-b200-32a2c29b3377",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['De groep bestaat uit Erika, een sexy krijgsvrouw, Arin de magiër, Lila de genezeres en Kael de boogschutter.',\n",
              " 'Het avontuursteam bevat de vurige krijgerin Maya, de wijze tovenaar Zorvath, deilstrijder Thrain en de roverin Elara.',\n",
              " 'De helden zijn Tirza de gezette zwaardvechtster, Ariniel de boogschutter, de genezer Joran en de zonderlinge magiër Xandros.',\n",
              " 'Het gezelschap bestaat uit de stoere krijgsvrouw Varda, de magiër Lyra, de genezeres Aria en de dappere ridder Thoric.']"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ],
      "source": [
        "pregenerate_party_in_language(\"Dutch\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uGUGJ7kbKGRQ"
      },
      "source": [
        "# Practice tasks\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fNgipjNwzFhu"
      },
      "source": [
        "##  Task 1. Character localization\n",
        "\n",
        "Let's add localization to our simple chat NPC class from Topic 1.\n",
        "\n",
        "Your task will be to implement the following localized chat pipeline:\n",
        "- The user's input is translated into English,\n",
        "- The NPC answers an English query in English (already implemented)\n",
        "- The NPC's answer is translated into the target language, and the translation is returned to the user.\n",
        "\n",
        "Here's all the code for character creation we used before. Add new code in necessary places"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "tdMbiABGzhA8"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "#with open(\"nebius_api_key\", \"r\") as file:\n",
        "#    nebius_api_key = file.read().strip()\n",
        "\n",
        "#os.environ[\"NEBIUS_API_KEY\"] = nebius_api_key\n",
        "from google.colab import userdata\n",
        "os.environ[\"NEBIUS_API_KEY\"] = userdata.get(\"nebius_api_key\")\n",
        "\n",
        "from collections import defaultdict, deque\n",
        "from openai import OpenAI\n",
        "from typing import Dict, Any, Optional\n",
        "import datetime\n",
        "import string\n",
        "import random\n",
        "from dataclasses import dataclass\n",
        "\n",
        "@dataclass\n",
        "class NPCConfig:\n",
        "    world_description: str\n",
        "    character_description: str\n",
        "    history_size: int = 10\n",
        "    has_scratchpad: bool = False\n",
        "\n",
        "class NPCFactoryError(Exception):\n",
        "    \"\"\"Base exception class for NPC Factory errors.\"\"\"\n",
        "    pass\n",
        "\n",
        "class NPCNotFoundError(NPCFactoryError):\n",
        "    \"\"\"Raised when trying to interact with a non-existent NPC.\"\"\"\n",
        "    def __init__(self, npc_id: str):\n",
        "        self.npc_id = npc_id\n",
        "        super().__init__(f\"NPC with ID '{npc_id}' not found\")\n",
        "\n",
        "class SimpleChatNPC:\n",
        "    def __init__(self, client: OpenAI, model: str, config: NPCConfig):\n",
        "        self.client = client\n",
        "        self.model = model\n",
        "        self.config = config\n",
        "        self.chat_histories = defaultdict(lambda: deque(maxlen=config.history_size))\n",
        "\n",
        "    def localize_input(self, input_text: str) -> str:\n",
        "        \"\"\"Translate user input into English.\"\"\"\n",
        "        return self.client.chat.completions.create(\n",
        "            messages=[\n",
        "                {\n",
        "                    'role': 'user',\n",
        "                    'content': f'Translate the following text into English:\\n{input_text}. Only output the translation itself.'\n",
        "                }\n",
        "            ],\n",
        "            model=self.model,\n",
        "        ).choices[0].message.content\n",
        "\n",
        "    def localize_output(self, output_text: str, target_language: str) -> str:\n",
        "        \"\"\"Translate the output into the target language.\"\"\"\n",
        "        return self.client.chat.completions.create(\n",
        "            messages=[\n",
        "                {\n",
        "                    'role': 'user',\n",
        "                    'content': f'Translate the following text into {target_language}:\\n{output_text}. Only output the translation itself.'\n",
        "                }\n",
        "            ],\n",
        "            model=self.model,\n",
        "        ).choices[0].message.content\n",
        "\n",
        "    def get_system_message(self) -> Dict[str, str]:\n",
        "        \"\"\"Returns the system message that defines the NPC's behavior.\"\"\"\n",
        "        character_description = self.config.character_description\n",
        "\n",
        "        if self.config.has_scratchpad:\n",
        "            character_description += \"\"\"\n",
        "You can use scratchpad for thinking before you answer: whatever you output between #SCRATCHPAD and #ANSWER won't be shown to anyone.\n",
        "You start your output with #SCRATCHPAD and after you've done thinking, you #ANSWER\"\"\"\n",
        "\n",
        "        return {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": f\"\"\"WORLD SETTING: {self.config.world_description}\n",
        "###\n",
        "{character_description}\"\"\"\n",
        "        }\n",
        "\n",
        "    def chat(self, user_message: str, user_id: str, user_language: str) -> str:\n",
        "        \"\"\"Process a user message and return the NPC's response.\"\"\"\n",
        "        messages = [self.get_system_message()]\n",
        "\n",
        "        # Add conversation history\n",
        "        history = list(self.chat_histories[user_id])\n",
        "        if history:\n",
        "            messages.extend(history)\n",
        "\n",
        "        # Add new user message\n",
        "        if user_language != \"English\":\n",
        "            user_message_dict = {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": self.localize_input(user_message)\n",
        "            }\n",
        "        else:\n",
        "            user_message_dict = {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": user_message\n",
        "            }\n",
        "        self.chat_histories[user_id].append(user_message_dict)\n",
        "        messages.append(user_message_dict)\n",
        "\n",
        "        try:\n",
        "            completion = self.client.chat.completions.create(\n",
        "                model=self.model,\n",
        "                messages=messages,\n",
        "                temperature=0.6\n",
        "            )\n",
        "\n",
        "            response = completion.choices[0].message.content\n",
        "\n",
        "            # Handle scratchpad if enabled\n",
        "            response_clean = response\n",
        "            if self.config.has_scratchpad:\n",
        "                import re\n",
        "                scratchpad_match = re.search(r\"#SCRATCHPAD(:?)(.*?)#ANSWER(:?)\", response, re.DOTALL)\n",
        "                if scratchpad_match:\n",
        "                    response_clean = response[scratchpad_match.end():].strip()\n",
        "\n",
        "            if user_language != \"English\":\n",
        "                response_clean = self.localize_output(response_clean, user_language)\n",
        "\n",
        "            # Store response in history, including the scratchpad\n",
        "            self.chat_histories[user_id].append({\n",
        "                \"role\": \"assistant\",\n",
        "                \"content\": response\n",
        "            })\n",
        "\n",
        "            # Return the message to the user without a scratchpad\n",
        "            return response_clean\n",
        "\n",
        "        except Exception as e:\n",
        "            return f\"Error: {str(e)}\"\n",
        "\n",
        "class NPCFactory:\n",
        "    def __init__(self, client: OpenAI, model: str):\n",
        "        self.client = client\n",
        "        self.model = model\n",
        "        self.npcs: Dict[str, SimpleChatNPC] = {}\n",
        "        self.user_ids: Dict[str, str] = {}  # username -> user_id mapping\n",
        "        self.user_preffered_languages: Dict[str, str] = {}  # user_id -> language mapping\n",
        "\n",
        "    def set_user_language(self, user_id: str, language: str):\n",
        "        \"\"\"Set the preferred language for a user.\"\"\"\n",
        "        self.user_preffered_languages[user_id] = language\n",
        "\n",
        "    def generate_id(self) -> str:\n",
        "        \"\"\"Generate a random unique identifier.\"\"\"\n",
        "        return ''.join(random.choice(string.ascii_letters) for _ in range(8))\n",
        "\n",
        "    def register_user(self, username: str) -> str:\n",
        "        \"\"\"Register a new user and return their unique ID.\n",
        "        If username already exists, appends a numerical suffix.\"\"\"\n",
        "        base_username = username\n",
        "        suffix = 1\n",
        "\n",
        "        # Keep trying with incremented suffixes until we find an unused name\n",
        "        while username in self.user_ids:\n",
        "            username = f\"{base_username}_{suffix}\"\n",
        "            suffix += 1\n",
        "\n",
        "        user_id = self.generate_id()\n",
        "        self.user_ids[username] = user_id\n",
        "        return user_id\n",
        "\n",
        "    def register_npc(self, world_description: str, character_description: str,\n",
        "                     history_size: int = 10, has_scratchpad: bool = False) -> str:\n",
        "        \"\"\"Create and register a new NPC, returning its unique ID.\"\"\"\n",
        "        npc_id = self.generate_id()\n",
        "\n",
        "        config = NPCConfig(\n",
        "            world_description=world_description,\n",
        "            character_description=character_description,\n",
        "            history_size=history_size,\n",
        "            has_scratchpad=has_scratchpad\n",
        "        )\n",
        "\n",
        "        self.npcs[npc_id] = SimpleChatNPC(self.client, self.model, config)\n",
        "        return npc_id\n",
        "\n",
        "    def chat_with_npc(self, npc_id: str, user_id: str, message: str) -> str:\n",
        "        \"\"\"Send a message to a specific NPC from a specific user.\n",
        "\n",
        "        Args:\n",
        "            npc_id: The unique identifier of the NPC\n",
        "            user_id: The unique identifier of the user\n",
        "            message: The message to send\n",
        "\n",
        "        Returns:\n",
        "            The NPC's response\n",
        "\n",
        "        Raises:\n",
        "            NPCNotFoundError: If the specified NPC doesn't exist\n",
        "        \"\"\"\n",
        "        user_language = self.user_preffered_languages.get(user_id, \"English\")\n",
        "        if npc_id not in self.npcs:\n",
        "            raise NPCNotFoundError(npc_id)\n",
        "\n",
        "        npc = self.npcs[npc_id]\n",
        "        return npc.chat(message, user_id, user_language=self.user_preffered_languages[user_id])\n",
        "\n",
        "    def get_npc_chat_history(self, npc_id: str, user_id: str) -> list:\n",
        "        \"\"\"Retrieve chat history between a specific user and NPC.\n",
        "\n",
        "        Args:\n",
        "            npc_id: The unique identifier of the NPC\n",
        "            user_id: The unique identifier of the user\n",
        "\n",
        "        Returns:\n",
        "            List of message dictionaries containing the chat history\n",
        "\n",
        "        Raises:\n",
        "            NPCNotFoundError: If the specified NPC doesn't exist\n",
        "        \"\"\"\n",
        "        if npc_id not in self.npcs:\n",
        "            raise NPCNotFoundError(npc_id)\n",
        "\n",
        "        return list(self.npcs[npc_id].chat_histories[user_id])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "C9za0tPtaGoQ"
      },
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "# Nebius uses the same OpenAI() class, but with additional details\n",
        "client = OpenAI(\n",
        "    base_url=\"https://api.studio.nebius.ai/v1/\",\n",
        "    api_key=os.environ.get(\"NEBIUS_API_KEY\"),\n",
        ")\n",
        "\n",
        "model = \"meta-llama/Meta-Llama-3.1-405B-Instruct\"\n",
        "\n",
        "# Creating a factory\n",
        "npc_factory = NPCFactory(client=client, model=model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "NjNokneB2P4k"
      },
      "outputs": [],
      "source": [
        "# Register a user\n",
        "user_id = npc_factory.register_user(\"Alice\")\n",
        "\n",
        "# Set user preffered language\n",
        "preffered_language = \"Old English\"\n",
        "npc_factory.set_user_language(user_id, preffered_language)\n",
        "\n",
        "# Create an NPC\n",
        "npc_id = npc_factory.register_npc(\n",
        "    world_description=\"Medieval London, XIII century\",\n",
        "    character_description=\"A knight at Edward I's court\",\n",
        "    has_scratchpad=False\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "DpWeu8623GRB"
      },
      "outputs": [],
      "source": [
        "def prettify_string(text, max_line_length=80):\n",
        "    \"\"\"Prints a string with line breaks at spaces to prevent horizontal scrolling.\n",
        "\n",
        "    Args:\n",
        "        text: The string to print.\n",
        "        max_line_length: The maximum length of each line.\n",
        "    \"\"\"\n",
        "\n",
        "    output_lines = []\n",
        "    lines = text.split(\"\\n\")\n",
        "    for line in lines:\n",
        "        current_line = \"\"\n",
        "        words = line.split()\n",
        "        for word in words:\n",
        "            if len(current_line) + len(word) + 1 <= max_line_length:\n",
        "                current_line += word + \" \"\n",
        "            else:\n",
        "                output_lines.append(current_line.strip())\n",
        "                current_line = word + \" \"\n",
        "        output_lines.append(current_line.strip())  # Append the last line\n",
        "    return \"\\n\".join(output_lines)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "vCcOYeRP3MG2",
        "outputId": "90524796-2d08-485c-efcc-f991da5604c0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Translated message: Hailo, hwā eart þu and hwæt bringeð þe hider?\n",
            "Original answer\n",
            "Gōd morgen þē. Ic eom Sir Eadweard de Muntfort, ān eadmod cniht on þēowdōme\n",
            "ūres mīclan Cyning Eadweard I. 'Tis wyrþ tō wician hēafod and þēowian þone\n",
            "crīadan. Ic hæbbe cōme tō Lundenbyrig tō bidden mīne arwyrþnesse þon cyning and\n",
            "tō tēonan on þone heah getil on his wyrþnesse.\n",
            "\n",
            "Sōþlīce, ic mūst andettan þæt ic eom hēr swylce tō sēcan giefþe mid þon cyning,\n",
            "þæt ic mæg beadon þone dōm mīnes hlǣford, þes Eorl of Lēgreceastre, sē þe hæbbe\n",
            "unrihtlīce benamed geboren fram sylfa hlēohan æt hēafod. Ic hōpe tō clǣnan his\n",
            "gōde naman and tō steren his wyrþ on þæs cyninges ēagan.\n",
            "\n",
            "Gebēad þē, sæg mē, hwā eart þū? Eart þū hlæford, cēpingmanna, oþþe hwilc sǣ\n",
            "oþer lique cleric? Hwæt bringþ þē tō þyss fæstan byrig?\n",
            "Translated answer\n",
            "Good morning to you. I am Sir Edward de Montfort, a humble knight in the\n",
            "service of our great King Edward I. It is worthy to bow your head and serve the\n",
            "crown. I have come to London to beg for my honor from the king and to claim the\n",
            "high seat of his worthiness.\n",
            "\n",
            "Truly, I must admit that I am here also to seek a gift from the king, that I\n",
            "may plead the case of my lord, the Earl of Leicester, who has been unjustly\n",
            "accused of treason by deceitful rumors at court. I hope to clear his good name\n",
            "and to increase his worth in the eyes of the king.\n",
            "\n",
            "I pray to you, tell me, who are you? Are you a lord, a merchant, or some other\n",
            "kind of cleric? What brings you to this fair city?\n"
          ]
        }
      ],
      "source": [
        "# We can hack our own code a bit and use translation features to test the system.\n",
        "\n",
        "npc = npc_factory.npcs[npc_id]\n",
        "\n",
        "message = \"Hello, who are you and what brings you here?\"\n",
        "message_translated = npc.localize_output(message, preffered_language)\n",
        "print(f\"Translated message: {message_translated}\")\n",
        "\n",
        "response = npc_factory.chat_with_npc(\n",
        "    npc_id,\n",
        "    user_id,\n",
        "    message_translated\n",
        ")\n",
        "print(\"Original answer\")\n",
        "print(prettify_string(response))\n",
        "print(\"Translated answer\")\n",
        "print(prettify_string(npc.localize_input(response)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eEGu6PFqjlrk"
      },
      "source": [
        "## Task 2. Translating poetry\n",
        "\n",
        "LLMs are notoriously bad at translating poetry. The resulting poems rarely have good rhyme and rhytm. Let's try to naively translate some Humpty Dumpty rhyme to a language of your choice:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "od4336JzjxYv",
        "outputId": "561aa36e-eafa-46ac-e730-44143306ea51",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here's the translation of the children's rhyme to Dutch:\n",
            "\n",
            "Humpty Dumpty zat op een muur,\n",
            "Humpty Dumpty maakte een grote val.\n",
            "Alle paarden van de koning en alle mannen van de koning\n",
            "Konden Humpty niet meer in elkaar zetten.\n",
            "\n",
            "Note: The name \"Humpty Dumpty\" is often left untranslated in Dutch, as it's a\n",
            "well-known character. If you want to translate the name as well, you could use\n",
            "\"Lammy Dommy\" or \"Eieltje\" (which means \"little egg\"), but \"Humpty Dumpty\" is\n",
            "more commonly used in Dutch.\n"
          ]
        }
      ],
      "source": [
        "language = \"Dutch\"\n",
        "poem = \"\"\"Humpty Dumpty sat on a wall,\n",
        "Humpty Dumpty had a great fall.\n",
        "All the king's horses and all the king's men\n",
        "Couldn't put Humpty together again.\"\"\"\n",
        "\n",
        "print(answer_with_llm(\n",
        "    f\"Translate the following children's rhyme to {language}\\n{poem}\"\n",
        "))"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zlzZJnG8VAMS"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cp832NFplDKN"
      },
      "source": [
        "This is hardly a good translation because the rhyme is completely broken.\n",
        "\n",
        "You task is to create a chain of LLM calls to do the following steps in translating a poem:\n",
        "\n",
        "1. Do a naive literal translation to preserve the meaning\n",
        "2. Rewrite the translation to retain rhyme and rhythm but perhaps loosing a bit of meaning.\n",
        "3. Finally have an editor look at both original and translation and make final touch ups.\n",
        "\n",
        "We encourage you to try and prompt your LLMs to do the job of \"Translator\", \"Editor\" and so on.\n",
        "Also choose the language you can understand best so that you can evaluate the result well (you can also change the original to some other language)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "jJvC7DthlG_x"
      },
      "outputs": [],
      "source": [
        "literal_translator_system_prompt = f\"\"\"\n",
        "You are a translator, whose task is to translate whatever you receive literally to {language}.\n",
        "You don't care if the text changes sentences, length, anything else, you only task is to retain as much meaning as possible\n",
        "and be as literal as possible in your translation.\n",
        "Output 'Translation:' and then the translation you created.\n",
        "\"\"\"\n",
        "\n",
        "rhyme_rewriter_system_prompt = f\"\"\"\n",
        "You are a rhyme writer. You task is to receive a text in {language} and rewrite\n",
        "it in a rhymed way also in {language}. It should have rhymes following one of the popular patterns,\n",
        "for example every other line, two and two and so on.\n",
        "You can distort the meaning a bit but not to lose it completely.\n",
        "Try to not make the text much longer.\n",
        "Output 'Rewriting:' and then the rewriting you created.\n",
        "\"\"\"\n",
        "\n",
        "editor_system_prompt = f\"\"\"\n",
        "You are an editor.\n",
        "You task is to inspect the the original text, naive translation and naive rewriting.\n",
        "Assess the quality of all of them and make finishing touch ups on the rhymed rewriting.\n",
        "Do not the text much longer than the original, it should have the same amount of lines.\n",
        "Do not output anything after the translation.\n",
        "Output the remarks you have for it in English and then\n",
        "Output 'Final translation:' and the final version of the translation to {language}\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "def translate_in_stages(input: str, language: str) -> str:\n",
        "    naive_translation = answer_with_llm(\n",
        "        system_prompt=literal_translator_system_prompt,\n",
        "        prompt=input\n",
        "    )\n",
        "    print(f\"\\n\\n{naive_translation}\\n\\n\")\n",
        "\n",
        "    rhymed_text = answer_with_llm(\n",
        "        system_prompt=rhyme_rewriter_system_prompt,\n",
        "        prompt=naive_translation\n",
        "    )\n",
        "    print(f\"\\n\\n{rhymed_text}\\n\\n\")\n",
        "\n",
        "    editor_notes = answer_with_llm(\n",
        "        system_prompt=editor_system_prompt,\n",
        "        prompt=f\"\"\"\n",
        "Original_text: {input}\n",
        "Naive translation: {naive_translation}\n",
        "Rhymed rewriting: {rhymed_text}\n",
        "\"\"\"\n",
        "    )\n",
        "    print(f\"\\n\\n{editor_notes}\\n\\n\")\n",
        "\n",
        "    return editor_notes.split(\"Final translation:\")[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "62i6CGh8oovG",
        "outputId": "bcfa7c87-36d6-4a15-b699-5d5b1d68789a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 695
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Translation:\n",
            "Humpty Dumpty zat op een muur,\n",
            "Humpty Dumpty had een grote val.\n",
            "Alle paarden van de koning en alle mannen van de koning\n",
            "Konden Humpty niet weer samen zetten.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Rewriting:\n",
            "Humpty Dumpty zat op een hoge muur zo steil,\n",
            "En maakte een val, wat een groteuil.\n",
            "De koning stuurde zijn paarden met spoed,\n",
            "En zijn mannen ook, maar Humpty bleef gebroken in de nood.\n",
            "Ze probeerden en probeerden, maar tevergeefs, helaas,\n",
            "Humpty Dumpty bleef stuk, en dat was zijn grote val, dat is waar.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "The original text is a well-known English nursery rhyme with a consistent rhyme\n",
            "scheme and meter.\n",
            "The naive translation is a literal translation of the original text, but it\n",
            "doesn't quite capture the same rhythm and rhyme scheme.\n",
            "The rhymed rewriting attempts to improve upon the naive translation by using a\n",
            "more natural-sounding Dutch phrase structure and incorporating rhymes, but it\n",
            "has some issues with meter and line length, and also adds an extra line that\n",
            "disrupts the original's concise structure.\n",
            "\n",
            "Final translation:\n",
            "Humpty Dumpty zat op een muur zo hoog,\n",
            "Humpty Dumpty had een grote val, een groot verdrietig lied.\n",
            "Alle paarden van de koning en alle mannen van de koning\n",
            "Konden Humpty niet weer samen zetten, hij bleef gebroken, dat is waar.\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nHumpty Dumpty zat op een muur zo hoog,\\nHumpty Dumpty had een grote val, een groot verdrietig lied.\\nAlle paarden van de koning en alle mannen van de koning\\nKonden Humpty niet weer samen zetten, hij bleef gebroken, dat is waar.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 54
        }
      ],
      "source": [
        "translate_in_stages(poem, language)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "spV0HJVSqgEe"
      },
      "source": [
        "## Task 3. Finding a hero\n",
        "\n",
        "Our kingdom has a very formal process for approving a hero for a specific quest.\n",
        "You task is to implement the approvement process using LLM calls:\n",
        "\n",
        "You receive a request for a hero and a description of a hero to hire for this quest.\n",
        "\n",
        "**Step 1**. Check that request is formally correct. You can come up with your own ideas, but we suggest the following criteria:\n",
        "- It has a name of the person requesting a hero and a date;\n",
        "- It has a description of who's going to supply the hero with money and other resources;\n",
        "- It has a reason why the hero is needed, some quest or challenge;\n",
        "- It has a recommended qualification for the hero;\n",
        "\n",
        "**Step 2**. Check that the problem with which the request is trying to deal is sufficient to actually find a hero, or perhaps an author might do it themself or find an easier solution.\n",
        "\n",
        "**Step 3**. Make sure that the description of the hero is compatible with the quest and requirements placed on the hero.\n",
        "\n",
        "These steps should be performed sequentially, one after another. If any of the stages fail, immediately return a refusal with justification - you don't want to waste any more compute on unworthy queries! If all the three steps succeed, return \"accepted\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "jrTho2N1o6hh"
      },
      "outputs": [],
      "source": [
        "formal_checker_system_prompt = \"\"\"\n",
        "You are presented with a request to hire a hero, check the following formalities:\n",
        "It has a name of the person requesting a hero and a date;\n",
        "It has a description of who's going to supply the hero with money and resource;\n",
        "It has a reason why the hero is needed, quest they are going to complete;\n",
        "It has a recommended qualifications for the hero;\n",
        "If any of those isn't true output the following:\n",
        "REFUSE: (here some brief justification why)\n",
        "If all of the criteria are good, output 'ACCEPT'\n",
        "\"\"\"\n",
        "\n",
        "problem_scale_checker = \"\"\"\n",
        "You are presented with a request to hire a hero.\n",
        "Access whether the problem described in the request is worthy of looking for a hero to solve.\n",
        "For example:\n",
        "Slaying a dragon - good\n",
        "Buying apples - bad\n",
        "Playing trumpet - bad\n",
        "Delivering sacred artifact - good.\n",
        "In general the problem should be quiet epic to be good.\n",
        "If it's not good output:\n",
        "REFUSE: (here briefly justify why itn't not fitting)\n",
        "If it's good output:\n",
        "'ACCEPT'\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "consistency_checker_prompt = \"\"\"\n",
        "You are presented with a request to hire a hero and the hero's description.\n",
        "You should make sure that whatever the problem described in the request\n",
        "can actually be solved by the hero proposed, based on the requirements in\n",
        "the request and the qualities of the hero presented.\n",
        "If the hero is fitting, output: 'ACCEPT'\n",
        "Otherwise output:\n",
        "REFUSE: (some brief justification why we reject the hero for this request)\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "def check_hero_request(request_for_a_hero: str, hero_descripition: str) -> str:\n",
        "    formal_check = answer_with_llm(\n",
        "        system_prompt=formal_checker_system_prompt,\n",
        "        prompt=request_for_a_hero\n",
        "    )\n",
        "    if \"REFUSE\" in formal_check:\n",
        "        return False, formal_check\n",
        "\n",
        "\n",
        "    problem_scale_check = answer_with_llm(\n",
        "        system_prompt=problem_scale_checker,\n",
        "        prompt=request_for_a_hero\n",
        "    )\n",
        "    if \"REFUSE\" in problem_scale_check:\n",
        "        return False, problem_scale_check\n",
        "\n",
        "    consistency_check = answer_with_llm(\n",
        "        system_prompt=consistency_checker_prompt,\n",
        "        prompt=f\"Request: {request_for_a_hero}, hero_description: {hero_descripition}\"\n",
        "    )\n",
        "    if \"REFUSE\" in consistency_check:\n",
        "        return False, consistency_check\n",
        "\n",
        "    return True, ''\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "CLYzKgtGs_LM"
      },
      "outputs": [],
      "source": [
        "epic_request = \"\"\"\n",
        "Epic Hero Request\n",
        "Name of Requestor: Lord Aeldric of the Silver Vale\n",
        "Date: The 15th Day of Bloomrise, Year 1025 of the Dawnstar Calendar\n",
        "\n",
        "Resource Provision:\n",
        "The hero shall be provisioned by the Guild of Eternal Flame, a conclave of wealthy artificers and arcane financiers, who have pledged a sum of 500,000 gold crowns, enchanted arms and armor, rare tomes of forgotten magic, a sky-bound griffon steed, and a personal aide skilled in healing and reconnaissance. All resources shall be delivered at the Hall of Summoning in Ironhold.\n",
        "\n",
        "Purpose of Request / Quest Description:\n",
        "Darkness stirs in the Hollow Spine Mountains, where the Obsidian Serpent — an ancient beast thought long dead — has risen anew. Villages lie in ruin, and the skies turn black with ash. The hero is summoned to descend into the Abyssal Breach, recover the lost Emberheart Crystal, and seal the rift before the World Spine fractures and all realms fall into chaos.\n",
        "\n",
        "Recommended Hero Qualifications:\n",
        "\n",
        "Proven mastery in combat, both arcane and martial\n",
        "\n",
        "Experience in surviving extreme environments and demonic incursions\n",
        "\n",
        "Wisdom enough to resist corruption, and strength to slay without hesitation\n",
        "\n",
        "Familiarity with ancient dialects and lost technologies\n",
        "\n",
        "A heart unwavering in the face of despair, and a spirit unbreakable by shadow\n",
        "\n",
        "Let the stars guide the right soul to answer. The fate of the realms balances on a blade’s edge.\n",
        "\"\"\"\n",
        "\n",
        "not_epic_request = \"\"\"\n",
        "Epic Hero Request\n",
        "Name of Requestor: Steve\n",
        "Date: 3rd of January 2025\n",
        "\n",
        "Resource Provision:\n",
        "I'll pay from my pocket\n",
        "\n",
        "Quest Description:\n",
        "I need someone to run to a supermarket for me, i'm hungry\n",
        "\n",
        "Required qualification:\n",
        "- Be very fast\n",
        "- Be smart to buy good snacks.\n",
        "\"\"\"\n",
        "\n",
        "wrong_request = \"\"\"\n",
        "Hello, I need a mighty warrior to slay evil, thank you!\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "IsXs8W5Puds7"
      },
      "outputs": [],
      "source": [
        "epic_hero = \"\"\"\n",
        "Hero Profile: Kaelen Thorne, the Ash-Wrought Sentinel\n",
        "\n",
        "Forged in the fires of the Blistering Wars and tempered by years wandering the haunted ruins of the Old Kingdoms, Kaelen Thorne is a battle-scarred veteran clad in rune-etched obsidian armor. With one eye gifted by the Seers of Valemire—able to glimpse the truth behind illusions—and a blade forged from a fallen star, Kaelen walks the line between light and shadow.\n",
        "\n",
        "Equal parts scholar and warrior, Kaelen speaks the tongues of forgotten realms and wields spells that twist the very air. Haunted but unyielding, Kaelen has turned away crowns and glory before—but for a quest that may decide the fate of all creation, the Sentinel rises once more.\n",
        "\"\"\"\n",
        "\n",
        "not_so_epic_hero = \"\"\"\n",
        "Tom the cat\n",
        "\n",
        "Is a cat, supposed to catch mice, but can't really do it.\n",
        "Has a lot of different surprising weapons and contraptions, but they always work against them.\n",
        "\n",
        "Works for cat food.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "8mXV0JPHvOc7",
        "outputId": "552ce386-a82e-499c-faed-701d2bed27ec",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(True, '')"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ],
      "source": [
        "check_hero_request(request_for_a_hero=epic_request, hero_descripition=epic_hero)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "WFfGVdWtwBm8",
        "outputId": "8c811ea8-e1d6-443c-c150-3a2f20b3f9d2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(False,\n",
              " \"REFUSE: The hero, Tom the cat, does not meet the recommended qualifications for\\nthe quest. Tom's inability to catch mice, a relatively simple task, raises\\nconcerns about his combat mastery and effectiveness in surviving extreme\\nenvironments. Additionally, there is no indication that Tom possesses the\\nnecessary wisdom, strength, or familiarity with ancient dialects and lost\\ntechnologies to complete the task. The fate of the realms requires a more\\ncapable and reliable hero.\")"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ],
      "source": [
        "check_hero_request(request_for_a_hero=epic_request, hero_descripition=not_so_epic_hero)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "mkT9U0vqwj_X",
        "outputId": "f9f6dc4c-9024-4cbd-cf6d-b9d91d8e2bc6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(False,\n",
              " \"REFUSE: The task of running to a supermarket to buy snacks is a mundane,\\neveryday chore that does not require heroic intervention. It lacks the epic\\nscale and significance typically associated with quests worthy of a hero's\\nattention.\")"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ],
      "source": [
        "check_hero_request(request_for_a_hero=not_epic_request, hero_descripition=epic_hero)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "6TeJWIkmwwHJ",
        "outputId": "22282fbf-8b98-4089-ab1d-9df5f7eff7d1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(False,\n",
              " 'REFUSE: The request lacks essential formalities, including the name of the\\nperson requesting a hero, a date, a description of who will supply the hero\\nwith money and resources, and recommended qualifications for the hero.')"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ],
      "source": [
        "check_hero_request(request_for_a_hero=wrong_request, hero_descripition=epic_hero)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lwYGvfTRVVjv"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "ca1ab29b864b4afe932bad700a994877": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d202a48f7d224a28afae3a46f36a3b4e",
              "IPY_MODEL_e9ff11204ad348b28d38c6a2815d62e9",
              "IPY_MODEL_eaa834edfef447bebb59175087d0fabc"
            ],
            "layout": "IPY_MODEL_87151a8dd49342b5b67caa8b61da6e93"
          }
        },
        "d202a48f7d224a28afae3a46f36a3b4e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c2ec9a653591428c8909b6ecfab7a761",
            "placeholder": "​",
            "style": "IPY_MODEL_244276ebc3c5459aa5ca9bf5a122e300",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "e9ff11204ad348b28d38c6a2815d62e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a1884290a5b044b8a565100d41f7cffe",
            "max": 3594,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b0b76f75336e4a0ab42758d352715dd8",
            "value": 3594
          }
        },
        "eaa834edfef447bebb59175087d0fabc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_14280a575e374bab9845a27c3149a5b2",
            "placeholder": "​",
            "style": "IPY_MODEL_078e22c4c9854283a11c0460cf8cd6ff",
            "value": " 3.59k/3.59k [00:00&lt;00:00, 84.8kB/s]"
          }
        },
        "87151a8dd49342b5b67caa8b61da6e93": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c2ec9a653591428c8909b6ecfab7a761": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "244276ebc3c5459aa5ca9bf5a122e300": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a1884290a5b044b8a565100d41f7cffe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b0b76f75336e4a0ab42758d352715dd8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "14280a575e374bab9845a27c3149a5b2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "078e22c4c9854283a11c0460cf8cd6ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3bf80215ce84430fb07ce1ab456ff38e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8633ca39588a4b2ca4e81447060721d6",
              "IPY_MODEL_c0098378e8f5450bbb69e7d65ef274e8",
              "IPY_MODEL_4768ce9c885446aeba6c7771567615b1"
            ],
            "layout": "IPY_MODEL_898f4d534bf64ab5aa987511c77e3a85"
          }
        },
        "8633ca39588a4b2ca4e81447060721d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_12254dc89ddc47f1b9967b3a0322ad01",
            "placeholder": "​",
            "style": "IPY_MODEL_cdbe437fe4c54f7eb6b15d0233541ff9",
            "value": "tokenizer.json: 100%"
          }
        },
        "c0098378e8f5450bbb69e7d65ef274e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d18657d9f2b849629d9a0da08c67d8b9",
            "max": 7847602,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6c7bcb0b30864f80a578863649fff9c3",
            "value": 7847602
          }
        },
        "4768ce9c885446aeba6c7771567615b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8cbd221849064af586c409f99b4f9753",
            "placeholder": "​",
            "style": "IPY_MODEL_675c53ed0e33494890879438f06e00ff",
            "value": " 7.85M/7.85M [00:01&lt;00:00, 7.23MB/s]"
          }
        },
        "898f4d534bf64ab5aa987511c77e3a85": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "12254dc89ddc47f1b9967b3a0322ad01": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cdbe437fe4c54f7eb6b15d0233541ff9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d18657d9f2b849629d9a0da08c67d8b9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6c7bcb0b30864f80a578863649fff9c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8cbd221849064af586c409f99b4f9753": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "675c53ed0e33494890879438f06e00ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "73a072d7b63b415098e10f1853664dab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2d1586852c7c44bdb20671a40246637e",
              "IPY_MODEL_8f0a6f514d7f4a43bac185753de7ba60",
              "IPY_MODEL_106060b961d74ac48e555082c6cd6347"
            ],
            "layout": "IPY_MODEL_d920d07dbaf744d29e5e1de8217b5881"
          }
        },
        "2d1586852c7c44bdb20671a40246637e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e210916cf612435c81e2aa2deca1e280",
            "placeholder": "​",
            "style": "IPY_MODEL_117ffc0559cc4c65ae08a7bcec021e5c",
            "value": "100%"
          }
        },
        "8f0a6f514d7f4a43bac185753de7ba60": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3717e6d8928c4039a1110a8c10f02734",
            "max": 11,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9cc39c3b77a1449da756071b69f2afce",
            "value": 11
          }
        },
        "106060b961d74ac48e555082c6cd6347": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9338e76d51be4aeabaae20a70bf5e130",
            "placeholder": "​",
            "style": "IPY_MODEL_d07207007c45438c826805cf1cdcfa00",
            "value": " 11/11 [03:03&lt;00:00, 18.69s/it]"
          }
        },
        "d920d07dbaf744d29e5e1de8217b5881": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e210916cf612435c81e2aa2deca1e280": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "117ffc0559cc4c65ae08a7bcec021e5c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3717e6d8928c4039a1110a8c10f02734": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9cc39c3b77a1449da756071b69f2afce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9338e76d51be4aeabaae20a70bf5e130": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d07207007c45438c826805cf1cdcfa00": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}